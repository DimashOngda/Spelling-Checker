{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KerasWorkingKazakh.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "I-sfCbepRe3h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "275a91f6-d1ac-496a-8e03-124b71553353"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sbiWEvI0RgNY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense, Dropout\n",
        "from keras import optimizers, metrics, backend as K\n",
        "\n",
        "VAL_MAXLEN = 16\n",
        "\n",
        "\n",
        "def truncated_acc(y_true, y_pred):\n",
        "    y_true = y_true[:, :VAL_MAXLEN, :]\n",
        "    y_pred = y_pred[:, :VAL_MAXLEN, :]\n",
        "    \n",
        "    acc = metrics.categorical_accuracy(y_true, y_pred)\n",
        "    return K.mean(acc, axis=-1)\n",
        "\n",
        "\n",
        "def truncated_loss(y_true, y_pred):\n",
        "    y_true = y_true[:, :VAL_MAXLEN, :]\n",
        "    y_pred = y_pred[:, :VAL_MAXLEN, :]\n",
        "    \n",
        "    loss = K.categorical_crossentropy(\n",
        "        target=y_true, output=y_pred, from_logits=False)\n",
        "    return K.mean(loss, axis=-1)\n",
        "\n",
        "\n",
        "def seq2seq(hidden_size, nb_input_chars, nb_target_chars):\n",
        "  \n",
        "    encoder_inputs = Input(shape=(None, nb_input_chars),\n",
        "                           name='encoder_data')\n",
        "    encoder_lstm = LSTM(hidden_size, recurrent_dropout=0.2,\n",
        "                        return_sequences=True, return_state=False,\n",
        "                        name='encoder_lstm_1')\n",
        "    encoder_outputs = encoder_lstm(encoder_inputs)\n",
        "    \n",
        "    encoder_lstm = LSTM(hidden_size, recurrent_dropout=0.2,\n",
        "                        return_sequences=False, return_state=True,\n",
        "                        name='encoder_lstm_2')\n",
        "    encoder_outputs, state_h, state_c = encoder_lstm(encoder_outputs)\n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "    decoder_inputs = Input(shape=(None, nb_target_chars),\n",
        "                           name='decoder_data')\n",
        "    \n",
        "    decoder_lstm = LSTM(hidden_size, dropout=0.2, return_sequences=True,\n",
        "                        return_state=True, name='decoder_lstm')\n",
        "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
        "                                         initial_state=encoder_states)\n",
        "    decoder_softmax = Dense(nb_target_chars, activation='softmax',\n",
        "                            name='decoder_softmax')\n",
        "    decoder_outputs = decoder_softmax(decoder_outputs)\n",
        "\n",
        "    \n",
        "    model = Model(inputs=[encoder_inputs, decoder_inputs],\n",
        "                  outputs=decoder_outputs)\n",
        "    \n",
        "    adam = optimizers.Adam(lr=0.001, decay=0.0)\n",
        "    model.compile(optimizer=adam, loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy', truncated_acc, truncated_loss])\n",
        "    \n",
        "    encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)\n",
        "\n",
        "    decoder_state_input_h = Input(shape=(hidden_size,))\n",
        "    decoder_state_input_c = Input(shape=(hidden_size,))\n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "        decoder_inputs, initial_state=decoder_states_inputs)\n",
        "    decoder_states = [state_h, state_c]\n",
        "    decoder_outputs = decoder_softmax(decoder_outputs)\n",
        "    decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs,\n",
        "                          outputs=[decoder_outputs] + decoder_states)\n",
        "\n",
        "    return model, encoder_model, decoder_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "clshRRXbRl5Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "480ad3c0-e21c-4d88-ad80-a1e2cbb65162"
      },
      "cell_type": "code",
      "source": [
        "!pip install unidecode"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/31/39/53096f9217b057cb049fe872b7fc7ce799a1a89b76cf917d9639e7a558b5/Unidecode-1.0.23-py2.py3-none-any.whl (237kB)\n",
            "\u001b[K    100% |████████████████████████████████| 245kB 9.1MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.0.23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "erq4skJeRmc7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "import unidecode\n",
        "import numpy as np\n",
        "\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input\n",
        "\n",
        "\n",
        "np.random.seed(1234)\n",
        "\n",
        "SOS = '\\t' \n",
        "EOS = '*' \n",
        "chars = list('йцукенгшщзхъфывапролджэячсмитьбюёәіңғүұқөһЙЦУКЕНГШЩЗХЪЭЖДЛОРПАВЫФЯЧСМИТЬБЮЁӘІҢҒҮҰҚӨҺ')\n",
        "REMOVE_CHARS = '[#$%\"\\+@<=>!&,-.?:;()*\\[\\]^_`{|}~/\\d\\t\\n\\r\\x0b\\x0c]'\n",
        "\n",
        "\n",
        "class CharacterTable(object):\n",
        "    \n",
        "    def __init__(self, chars):\n",
        "      self.chars = sorted(set(chars))\n",
        "      '''self.char2index = dict((c, i) for i, c in enumerate(self.chars))\n",
        "      self.index2char = dict((i, c) for i, c in enumerate(self.chars))\n",
        "      self.size = len(self.chars)'''\n",
        "      vocab_to_int = {}\n",
        "      count = 0\n",
        "      for character in self.chars:\n",
        "        if character not in vocab_to_int:\n",
        "          vocab_to_int[character] = count\n",
        "          count += 1\n",
        "      self.char2index = vocab_to_int\n",
        "      self.index2char = {v: k for k, v in self.char2index.items()}\n",
        "      #self.index2char = dict((i, c) for i, c in enumerate(self.CHARS))\n",
        "      self.size = len(self.chars)\n",
        "    \n",
        "    def encode(self, C, nb_rows):\n",
        "      \"\"\"One-hot encode given string C.\n",
        "        # Arguments\n",
        "          C: string, to be encoded.\n",
        "          nb_rows: Number of rows in the returned one-hot encoding. This is\n",
        "          used to keep the # of rows for each data the same via padding.\n",
        "        \"\"\"\n",
        "      x = np.zeros((nb_rows, len(self.chars)), dtype=np.float32)\n",
        "      for i, c in enumerate(C):\n",
        "        x[i, self.char2index[c]] = 1.0\n",
        "      return x\n",
        "    '''def encode(self, C, nb_rows):\n",
        "      onehot_encoded = []\n",
        "      for value in range(len(C)):\n",
        "        letter = [0 for _ in range(len(self.char2index))]\n",
        "        letter[value] = 1\n",
        "        onehot_encoded.append(letter)\n",
        "        onehotenc = np.array((onehot_encoded), dtype=np.float32)\n",
        "      return onehotenc''' \n",
        "    def decode(self, x, calc_argmax=True):\n",
        "        \"\"\"Decode the given vector or 2D array to their character output.\n",
        "        # Arguments\n",
        "          x: A vector or 2D array of probabilities or one-hot encodings,\n",
        "          or a vector of character indices (used with `calc_argmax=False`).\n",
        "          calc_argmax: Whether to find the character index with maximum\n",
        "          probability, defaults to `True`.\n",
        "        \"\"\"\n",
        "        if calc_argmax:\n",
        "            indices = x.argmax(axis=-1)\n",
        "        else:\n",
        "            indices = x\n",
        "        chars = ''.join(self.index2char[ind] for ind in indices)\n",
        "        return indices, chars\n",
        "\n",
        "    def sample_multinomial(self, preds, temperature=1.0):\n",
        "        \"\"\"Sample index and character output from `preds`,\n",
        "        an array of softmax probabilities with shape (1, 1, nb_chars).\n",
        "        \"\"\"\n",
        "        # Reshaped to 1D array of shape (nb_chars,).\n",
        "        preds = np.reshape(preds, len(self.chars)).astype(np.float64)\n",
        "        preds = np.log(preds) / temperature\n",
        "        exp_preds = np.exp(preds)\n",
        "        preds = exp_preds / np.sum(exp_preds)\n",
        "        probs = np.random.multinomial(1, preds, 1)\n",
        "        index = np.argmax(probs)\n",
        "        char  = self.index2char[index]\n",
        "        return index, char\n",
        "\n",
        "\n",
        "def read_text(data_path, list_of_books):\n",
        "    text = ''\n",
        "    for book in list_of_books:\n",
        "        file_path = os.path.join(data_path, book)\n",
        "        strings = open(file_path).read()\n",
        "        text += strings + ' '\n",
        "    return text\n",
        "\n",
        "\n",
        "def tokenize(text):\n",
        "    tokens = [re.sub(REMOVE_CHARS, '', token)\n",
        "              for token in re.split(\"[-\\n ]\", text)]\n",
        "    return tokens\n",
        "\n",
        "    \n",
        "def add_speling_erors(token, error_rate):\n",
        "    \n",
        "    assert(0.0 <= error_rate < 1.0)\n",
        "    if len(token) < 3:\n",
        "        return token\n",
        "    rand = np.random.rand()\n",
        "   \n",
        "    prob = error_rate / 4.0\n",
        "    if rand < prob:\n",
        "        random_char_index = np.random.randint(len(token))\n",
        "        token = token[:random_char_index] + np.random.choice(chars) \\\n",
        "                + token[random_char_index + 1:]\n",
        "    elif prob < rand < prob * 2:\n",
        "       \n",
        "        random_char_index = np.random.randint(len(token))\n",
        "        token = token[:random_char_index] + token[random_char_index + 1:]\n",
        "    elif prob * 2 < rand < prob * 3:\n",
        "       \n",
        "        random_char_index = np.random.randint(len(token))\n",
        "        token = token[:random_char_index] + np.random.choice(chars) \\\n",
        "                + token[random_char_index:]\n",
        "    elif prob * 3 < rand < prob * 4:\n",
        "        \n",
        "        random_char_index = np.random.randint(len(token) - 1)\n",
        "        token = token[:random_char_index]  + token[random_char_index + 1] \\\n",
        "                + token[random_char_index] + token[random_char_index + 2:]\n",
        "    else:\n",
        "  \n",
        "        pass\n",
        "    return token\n",
        "\n",
        "\n",
        "def transform(tokens, maxlen, error_rate=0.3, shuffle=True):\n",
        "    \"\"\"Transform tokens into model inputs and targets.\n",
        "    All inputs and targets are padded to maxlen with EOS character.\n",
        "    \"\"\"\n",
        "    if shuffle:\n",
        "        print('Shuffling data.')\n",
        "        np.random.shuffle(tokens)\n",
        "    encoder_tokens = []\n",
        "    decoder_tokens = []\n",
        "    target_tokens = []\n",
        "    for token in tokens:\n",
        "        encoder = add_speling_erors(token, error_rate=error_rate)\n",
        "        encoder += EOS * (maxlen - len(encoder)) # Padded to maxlen.\n",
        "        encoder_tokens.append(encoder)\n",
        "    \n",
        "        decoder = SOS + token\n",
        "        decoder += EOS * (maxlen - len(decoder))\n",
        "        decoder_tokens.append(decoder)\n",
        "    \n",
        "        target = decoder[1:]\n",
        "        target += EOS * (maxlen - len(target))\n",
        "        target_tokens.append(target)\n",
        "        \n",
        "        assert(len(encoder) == len(decoder) == len(target))\n",
        "    return encoder_tokens, decoder_tokens, target_tokens\n",
        "\n",
        "\n",
        "def batch(tokens, maxlen, ctable, batch_size=128, reverse=False):\n",
        "    \"\"\"Split data into chunks of `batch_size` examples.\"\"\"\n",
        "    def generate(tokens, reverse):\n",
        "        while(True): # This flag yields an infinite generator.\n",
        "            for token in tokens:\n",
        "                if reverse:\n",
        "                    token = token[::-1]\n",
        "                yield token\n",
        "    \n",
        "    token_iterator = generate(tokens, reverse)\n",
        "    data_batch = np.zeros((batch_size, maxlen, ctable.size),\n",
        "                          dtype=np.float32)\n",
        "    while(True):\n",
        "        for i in range(batch_size):\n",
        "            token = next(token_iterator)\n",
        "            data_batch[i] = ctable.encode(token, maxlen)\n",
        "        yield data_batch\n",
        "\n",
        "\n",
        "def datagen(encoder_iter, decoder_iter, target_iter):\n",
        "    \"\"\"Utility function to load data into required model format.\"\"\"\n",
        "    inputs = zip(encoder_iter, decoder_iter)\n",
        "    while(True):\n",
        "        encoder_input, decoder_input = next(inputs)\n",
        "        target = next(target_iter)\n",
        "        yield ([encoder_input, decoder_input], target)\n",
        "\n",
        "\n",
        "def decode_sequences(inputs, targets, input_ctable, target_ctable,\n",
        "                     maxlen, reverse, encoder_model, decoder_model,\n",
        "                     nb_examples, sample_mode='argmax', random=True):\n",
        "    input_tokens = []\n",
        "    target_tokens = []\n",
        "    \n",
        "    if random:\n",
        "        indices = np.random.randint(0, len(inputs), nb_examples)\n",
        "    else:\n",
        "        indices = range(nb_examples)\n",
        "        \n",
        "    for index in indices:\n",
        "        input_tokens.append(inputs[index])\n",
        "        target_tokens.append(targets[index])\n",
        "    input_sequences = batch(input_tokens, maxlen, input_ctable,\n",
        "                            nb_examples, reverse)\n",
        "    input_sequences = next(input_sequences)\n",
        "    \n",
        "    # Procedure for inference mode (sampling):\n",
        "    # 1) Encode input and retrieve initial decoder state.\n",
        "    # 2) Run one step of decoder with this initial state\n",
        "    #    and a start-of-sequence character as target.\n",
        "    #    Output will be the next target character.\n",
        "    # 3) Repeat with the current target character and current states.\n",
        "\n",
        "    # Encode the input as state vectors.    \n",
        "    states_value = encoder_model.predict(input_sequences)\n",
        "    \n",
        "    # Create batch of empty target sequences of length 1 character.\n",
        "    target_sequences = np.zeros((nb_examples, 1, target_ctable.size))\n",
        "    # Populate the first element of target sequence\n",
        "    # with the start-of-sequence character.\n",
        "    target_sequences[:, 0, target_ctable.char2index[SOS]] = 1.0\n",
        "\n",
        "    # Sampling loop for a batch of sequences.\n",
        "    # Exit condition: either hit max character limit\n",
        "    # or encounter end-of-sequence character.\n",
        "    decoded_tokens = [''] * nb_examples\n",
        "    for _ in range(maxlen):\n",
        "        # `char_probs` has shape\n",
        "        # (nb_examples, 1, nb_target_chars)\n",
        "        char_probs, h, c = decoder_model.predict(\n",
        "            [target_sequences] + states_value)\n",
        "\n",
        "        # Reset the target sequences.\n",
        "        target_sequences = np.zeros((nb_examples, 1, target_ctable.size))\n",
        "\n",
        "        # Sample next character using argmax or multinomial mode.\n",
        "        sampled_chars = []\n",
        "        for i in range(nb_examples):\n",
        "            if sample_mode == 'argmax':\n",
        "                next_index, next_char = target_ctable.decode(\n",
        "                    char_probs[i], calc_argmax=True)\n",
        "            elif sample_mode == 'multinomial':\n",
        "                next_index, next_char = target_ctable.sample_multinomial(\n",
        "                    char_probs[i], temperature=0.5)\n",
        "            else:\n",
        "                raise Exception(\n",
        "                    \"`sample_mode` accepts `argmax` or `multinomial`.\")\n",
        "            decoded_tokens[i] += next_char\n",
        "            sampled_chars.append(next_char) \n",
        "            # Update target sequence with index of next character.\n",
        "            target_sequences[i, 0, next_index] = 1.0\n",
        "\n",
        "        stop_char = set(sampled_chars)\n",
        "        if len(stop_char) == 1 and stop_char.pop() == EOS:\n",
        "            break\n",
        "            \n",
        "        # Update states.\n",
        "        states_value = [h, c]\n",
        "    \n",
        "    # Sampling finished.\n",
        "    input_tokens   = [re.sub('[%s]' % EOS, '', token)\n",
        "                      for token in input_tokens]\n",
        "    target_tokens  = [re.sub('[%s]' % EOS, '', token)\n",
        "                      for token in target_tokens]\n",
        "    decoded_tokens = [re.sub('[%s]' % EOS, '', token)\n",
        "                      for token in decoded_tokens]\n",
        "    return input_tokens, target_tokens, decoded_tokens\n",
        "\n",
        "\n",
        "def restore_model(path_to_full_model, hidden_size):\n",
        "    \"\"\"Restore model to construct the encoder and decoder.\"\"\"\n",
        "    model = load_model(path_to_full_model, custom_objects={\n",
        "        'truncated_acc': truncated_acc, 'truncated_loss': truncated_loss})\n",
        "    \n",
        "    encoder_inputs = model.input[0] # encoder_data\n",
        "    encoder_lstm1 = model.get_layer('encoder_lstm_1')\n",
        "    encoder_lstm2 = model.get_layer('encoder_lstm_2')\n",
        "    \n",
        "    encoder_outputs = encoder_lstm1(encoder_inputs)\n",
        "    _, state_h, state_c = encoder_lstm2(encoder_outputs)\n",
        "    encoder_states = [state_h, state_c]\n",
        "    encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)\n",
        "\n",
        "    decoder_inputs = model.input[1] # decoder_data\n",
        "    decoder_state_input_h = Input(shape=(hidden_size,))\n",
        "    decoder_state_input_c = Input(shape=(hidden_size,))\n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "    decoder_lstm = model.get_layer('decoder_lstm')\n",
        "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "        decoder_inputs, initial_state=decoder_states_inputs)\n",
        "    decoder_states = [state_h, state_c]\n",
        "    decoder_softmax = model.get_layer('decoder_softmax')\n",
        "    decoder_outputs = decoder_softmax(decoder_outputs)\n",
        "    decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs,\n",
        "                          outputs=[decoder_outputs] + decoder_states)\n",
        "    return encoder_model, decoder_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3NoppBtdRqfu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17853
        },
        "outputId": "29772318-c407-4260-e659-f88eb3383735"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(1234)\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "# from utils import CharacterTable, transform\n",
        "# from utils import batch, datagen, decode_sequences\n",
        "# from utils import read_text, tokenize\n",
        "# from model import seq2seq\n",
        "\n",
        "error_rate = 0.8\n",
        "hidden_size = 512\n",
        "nb_epochs = 100\n",
        "train_batch_size = 128\n",
        "val_batch_size = 256\n",
        "sample_mode = 'argmax'\n",
        "# Input sequences may optionally be reversed,\n",
        "# shown to increase performance by introducing\n",
        "# shorter term dependencies between source and target:\n",
        "# \"Learning to Execute\"\n",
        "# http://arxiv.org/abs/1410.4615\n",
        "# \"Sequence to Sequence Learning with Neural Networks\"\n",
        "# https://arxiv.org/abs/1409.3215\n",
        "reverse = True\n",
        "\n",
        "data_path = '/content/drive/My Drive/Colab Notebooks/kazakhBooks' \n",
        "books = ['txt1.txt','txt2.txt','txt3.txt','txt4.txt','txt5.txt',\n",
        "               'txt6.txt','txt7.txt','txt8.txt','txt9.txt']\n",
        "val_books = ['txt1.txt']\n",
        "#test_sentence = 'Уақыт қана емдейтiн, ұзап барып айығатын, әзiрше үзiлмес шер'\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Prepare training data.\n",
        "    text  = read_text(data_path, books)\n",
        "    vocab = tokenize(text)\n",
        "    vocab = list(filter(None, set(vocab)))\n",
        "    \n",
        "    # `maxlen` is the length of the longest word in the vocabulary\n",
        "    # plus two SOS and EOS characters.\n",
        "    maxlen = max([len(token) for token in vocab]) + 2\n",
        "    train_encoder, train_decoder, train_target = transform(vocab, maxlen, error_rate=error_rate, shuffle=False)\n",
        "    \n",
        "    print(train_encoder[:10])\n",
        "    print(train_decoder[:10])\n",
        "    print(train_target[:10])\n",
        "\n",
        "    input_chars = set(' '.join(train_encoder))\n",
        "    target_chars = set(' '.join(train_decoder))\n",
        "    nb_input_chars = len(input_chars)\n",
        "    nb_target_chars = len(target_chars)\n",
        "\n",
        "    print('Size of training vocabulary =', len(vocab))\n",
        "    print('Number of unique input characters:', nb_input_chars)\n",
        "    print('Number of unique target characters:', nb_target_chars)\n",
        "    print('Max sequence length in the training set:', maxlen)\n",
        "\n",
        "    # Prepare validation data.\n",
        "    text = read_text(data_path, val_books)\n",
        "    val_tokens = tokenize(text)\n",
        "    val_tokens = list(filter(None, val_tokens))\n",
        "\n",
        "    val_maxlen = max([len(token) for token in val_tokens]) + 2\n",
        "    val_encoder, val_decoder, val_target = transform(\n",
        "        val_tokens, maxlen, error_rate=error_rate, shuffle=False)\n",
        "    print(val_encoder[:10])\n",
        "    print(val_decoder[:10])\n",
        "    print(val_target[:10])\n",
        "    print('Number of non-unique validation tokens =', len(val_tokens))\n",
        "    print('Max sequence length in the validation set:', val_maxlen)\n",
        "\n",
        "    # Define training and evaluation configuration.\n",
        "    input_ctable  = CharacterTable(input_chars)\n",
        "    target_ctable = CharacterTable(target_chars)\n",
        "\n",
        "    train_steps = len(vocab) // train_batch_size\n",
        "    val_steps = len(val_tokens) // val_batch_size\n",
        "\n",
        "    # Compile the model.\n",
        "    model, encoder_model, decoder_model = seq2seq(\n",
        "        hidden_size, nb_input_chars, nb_target_chars)\n",
        "    print(model.summary())\n",
        "\n",
        "    # Train and evaluate.\n",
        "    for epoch in range(nb_epochs):\n",
        "        print('Main Epoch {:d}/{:d}'.format(epoch + 1, nb_epochs))\n",
        "    \n",
        "        train_encoder, train_decoder, train_target = transform(\n",
        "            vocab, maxlen, error_rate=error_rate, shuffle=True)\n",
        "        \n",
        "        train_encoder_batch = batch(train_encoder, maxlen, input_ctable,\n",
        "                                    train_batch_size, reverse)\n",
        "        train_decoder_batch = batch(train_decoder, maxlen, target_ctable,\n",
        "                                    train_batch_size)\n",
        "        train_target_batch  = batch(train_target, maxlen, target_ctable,\n",
        "                                    train_batch_size)    \n",
        "\n",
        "        val_encoder_batch = batch(val_encoder, maxlen, input_ctable,\n",
        "                                  val_batch_size, reverse)\n",
        "        val_decoder_batch = batch(val_decoder, maxlen, target_ctable,\n",
        "                                  val_batch_size)\n",
        "        val_target_batch  = batch(val_target, maxlen, target_ctable,\n",
        "                                  val_batch_size)\n",
        "    \n",
        "        train_loader = datagen(train_encoder_batch,\n",
        "                               train_decoder_batch, train_target_batch)\n",
        "        val_loader = datagen(val_encoder_batch,\n",
        "                             val_decoder_batch, val_target_batch)\n",
        "    \n",
        "        model.fit_generator(train_loader,\n",
        "                            steps_per_epoch=train_steps,\n",
        "                            epochs=1, verbose=1,\n",
        "                            validation_data=val_loader,\n",
        "                            validation_steps=val_steps)\n",
        "\n",
        "        # On epoch end - decode a batch of misspelled tokens from the\n",
        "        # validation set to visualize speller performance.\n",
        "        nb_tokens = 5\n",
        "        input_tokens, target_tokens, decoded_tokens = decode_sequences(\n",
        "            val_encoder, val_target, input_ctable, target_ctable,\n",
        "            maxlen, reverse, encoder_model, decoder_model, nb_tokens,\n",
        "            sample_mode=sample_mode, random=True)\n",
        "        \n",
        "        print('-')\n",
        "        print('Input tokens:  ', input_tokens)\n",
        "        print('Decoded tokens:', decoded_tokens)\n",
        "        print('Target tokens: ', target_tokens)\n",
        "        print('-')\n",
        "        \n",
        "        # Save the model at end of each epoch.\n",
        "        model_file = '_'.join(['seq2seq', 'epoch', str(epoch + 1)]) + '.h5'\n",
        "        save_dir = 'checkpoints'\n",
        "        if not os.path.exists(save_dir):\n",
        "            os.makedirs(save_dir)\n",
        "        save_path = os.path.join(save_dir, model_file)\n",
        "        print('Saving full model to {:s}'.format(save_path))\n",
        "        model.save(save_path)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['зуылдаЪ*******************', 'аДедім********************', 'Сүйiп*********************', 'араларЦ*******************', 'маңлық********************', 'БөкеншОнің****************', 'БолғИнда******************', 'рөл***********************', 'Қытың*********************', 'айтқаныңыздаүй************']\n",
            "['\\tзуылдап******************', '\\tДедім********************', '\\tСүйiп********************', '\\tаралары******************', '\\tмаңлық*******************', '\\tБөкеншінің***************', '\\tБолғанда*****************', '\\tрөлі*********************', '\\tҚыстың*******************', '\\tайтқаныңыздай************']\n",
            "['зуылдап*******************', 'Дедім*********************', 'Сүйiп*********************', 'аралары*******************', 'маңлық********************', 'Бөкеншінің****************', 'Болғанда******************', 'рөлі**********************', 'Қыстың********************', 'айтқаныңыздай*************']\n",
            "Size of training vocabulary = 48101\n",
            "Number of unique input characters: 117\n",
            "Number of unique target characters: 115\n",
            "Max sequence length in the training set: 26\n",
            "['\\ufeffТаұ**********************', 'алдында*******************', 'бДiр**********************', 'ғна***********************', 'саағт*********************', 'мызығаны******************', 'бщлмаса*******************', 'Өбай**********************', 'бӘл***********************', 'түндБ*********************']\n",
            "['\\t\\ufeffТаң*********************', '\\tалдында******************', '\\tбiр**********************', '\\tғана*********************', '\\tсағат********************', '\\tмызғығаны****************', '\\tболмаса******************', '\\tАбай*********************', '\\tбұл**********************', '\\tтүндi********************']\n",
            "['\\ufeffТаң**********************', 'алдында*******************', 'бiр***********************', 'ғана**********************', 'сағат*********************', 'мызғығаны*****************', 'болмаса*******************', 'Абай**********************', 'бұл***********************', 'түндi*********************']\n",
            "Number of non-unique validation tokens = 128753\n",
            "Max sequence length in the validation set: 20\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "encoder_data (InputLayer)       (None, None, 117)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "encoder_lstm_1 (LSTM)           (None, None, 512)    1290240     encoder_data[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "decoder_data (InputLayer)       (None, None, 115)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "encoder_lstm_2 (LSTM)           [(None, 512), (None, 2099200     encoder_lstm_1[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "decoder_lstm (LSTM)             [(None, None, 512),  1286144     decoder_data[0][0]               \n",
            "                                                                 encoder_lstm_2[0][1]             \n",
            "                                                                 encoder_lstm_2[0][2]             \n",
            "__________________________________________________________________________________________________\n",
            "decoder_softmax (Dense)         (None, None, 115)    58995       decoder_lstm[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 4,734,579\n",
            "Trainable params: 4,734,579\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Main Epoch 1/100\n",
            "Shuffling data.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 68s 182ms/step - loss: 1.0323 - acc: 0.7373 - truncated_acc: 0.5751 - truncated_loss: 1.6472 - val_loss: 0.6479 - val_acc: 0.8257 - val_truncated_acc: 0.7168 - val_truncated_loss: 1.0526\n",
            "-\n",
            "Input tokens:   ['қиЛятын', 'Баймағамбет', 'не', 'губернтаоры', 'етi']\n",
            "Decoded tokens: ['қаланын', 'қараланын', 'керен', 'қараланды', 'керен']\n",
            "Target tokens:  ['қиятын', 'Баймағамбет', 'не', 'губернаторы', 'еттi']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_1.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/network.py:877: UserWarning: Layer decoder_lstm was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'encoder_lstm_2/while/Exit_2:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'encoder_lstm_2/while/Exit_3:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
            "  '. They will not be included '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Main Epoch 2/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 64s 171ms/step - loss: 0.8803 - acc: 0.7645 - truncated_acc: 0.6176 - truncated_loss: 1.4286 - val_loss: 0.4887 - val_acc: 0.8787 - val_truncated_acc: 0.8028 - val_truncated_loss: 0.7940\n",
            "-\n",
            "Input tokens:   ['Жаны', 'Женесiне', 'айтады', 'қанды', 'едВi']\n",
            "Decoded tokens: ['қары', 'сесенен', 'айтары', 'қарын', 'еле']\n",
            "Target tokens:  ['Жаны', 'енесiне', 'айтады', 'қанды', 'едi']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_2.h5\n",
            "Main Epoch 3/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 65s 175ms/step - loss: 0.7518 - acc: 0.8133 - truncated_acc: 0.6968 - truncated_loss: 1.2206 - val_loss: 0.3871 - val_acc: 0.9159 - val_truncated_acc: 0.8634 - val_truncated_loss: 0.6289\n",
            "-\n",
            "Input tokens:   ['айТтсын', 'жарасын', 'болсыына', '–', 'Тұрғанбсайды']\n",
            "Decoded tokens: ['айтыны', 'жарасын', 'болысын', 'бар', 'Тұрғандайды']\n",
            "Target tokens:  ['айтсын', 'жарасын', 'болысына', '–', 'Тұрғанбайды']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_3.h5\n",
            "Main Epoch 4/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 64s 172ms/step - loss: 0.7140 - acc: 0.8314 - truncated_acc: 0.7262 - truncated_loss: 1.1590 - val_loss: 0.3444 - val_acc: 0.9279 - val_truncated_acc: 0.8829 - val_truncated_loss: 0.5595\n",
            "-\n",
            "Input tokens:   ['Әбдiрахмын', 'ол', 'бөолар', 'санды', 'Дiдлә']\n",
            "Decoded tokens: ['Әбiрiмен', 'олы', 'болары', 'санды', 'Әiлдi']\n",
            "Target tokens:  ['Әбдiрахман', 'ол', 'болар', 'қанды', 'Дiлдә']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_4.h5\n",
            "Main Epoch 5/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 64s 170ms/step - loss: 0.6818 - acc: 0.8428 - truncated_acc: 0.7448 - truncated_loss: 1.1067 - val_loss: 0.3451 - val_acc: 0.9311 - val_truncated_acc: 0.8880 - val_truncated_loss: 0.5605\n",
            "-\n",
            "Input tokens:   ['брi', 'сзөiм', 'байлпа', 'ӘМенi', 'рбол']\n",
            "Decoded tokens: ['бiрi', 'сөзiм', 'байлап', 'Менi', 'болы']\n",
            "Target tokens:  ['бiр', 'сөзiм', 'байлап', 'Менi', 'Ербол']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_5.h5\n",
            "Main Epoch 6/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 64s 171ms/step - loss: 0.6349 - acc: 0.8550 - truncated_acc: 0.7646 - truncated_loss: 1.0306 - val_loss: 0.3208 - val_acc: 0.9349 - val_truncated_acc: 0.8942 - val_truncated_loss: 0.5210\n",
            "-\n",
            "Input tokens:   ['Длә', 'тығыла', 'елдн', 'бiрi', 'көзiд']\n",
            "Decoded tokens: ['Дәл', 'тығыла', 'елден', 'бiрi', 'көзi']\n",
            "Target tokens:  ['Дәл', 'тығыла', 'елден', 'бiрi', 'көздi']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_6.h5\n",
            "Main Epoch 7/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 64s 171ms/step - loss: 0.6568 - acc: 0.8496 - truncated_acc: 0.7557 - truncated_loss: 1.0663 - val_loss: 0.3471 - val_acc: 0.9299 - val_truncated_acc: 0.8861 - val_truncated_loss: 0.5640\n",
            "-\n",
            "Input tokens:   ['жiiгт', 'тастады', 'үтлкi', 'спар', 'еЗндi']\n",
            "Decoded tokens: ['жiрiге', 'тастады', 'үйлiк', 'сапар', 'ерiндi']\n",
            "Target tokens:  ['жiгiт', 'тастады', 'түлкi', 'сапар', 'ендi']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_7.h5\n",
            "Main Epoch 8/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 65s 172ms/step - loss: 0.6467 - acc: 0.8511 - truncated_acc: 0.7582 - truncated_loss: 1.0499 - val_loss: 0.2985 - val_acc: 0.9386 - val_truncated_acc: 0.9002 - val_truncated_loss: 0.4851\n",
            "-\n",
            "Input tokens:   ['уайытм', 'брi', 'Екб', 'Мйабасар', 'зӘдiлет']\n",
            "Decoded tokens: ['уайым', 'бiрi', 'Екбе', 'Майбасар', 'Әздiлет']\n",
            "Target tokens:  ['уайым', 'бiр', 'Екi', 'Майбасар', 'Әдiлет']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_8.h5\n",
            "Main Epoch 9/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 64s 170ms/step - loss: 0.8758 - acc: 0.7807 - truncated_acc: 0.6455 - truncated_loss: 1.3929 - val_loss: 0.5692 - val_acc: 0.8469 - val_truncated_acc: 0.7513 - val_truncated_loss: 0.9246\n",
            "-\n",
            "Input tokens:   ['келге', 'ьтедi', 'қаплды', 'төргi', 'қрытты']\n",
            "Decoded tokens: ['келектер', 'телен', 'қаласын', 'көрені', 'қарастан']\n",
            "Target tokens:  ['келген', 'етедi', 'қалды', 'төргi', 'қорытты']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_9.h5\n",
            "Main Epoch 10/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 63s 168ms/step - loss: 0.7754 - acc: 0.7928 - truncated_acc: 0.6635 - truncated_loss: 1.2591 - val_loss: 0.4248 - val_acc: 0.8958 - val_truncated_acc: 0.8307 - val_truncated_loss: 0.6902\n",
            "-\n",
            "Input tokens:   ['от', 'дойында', 'үн', 'кеЭiншек', 'зжүзiне']\n",
            "Decoded tokens: ['отын', 'ойнындай', 'түн', 'кешiншi', 'жүздерi']\n",
            "Target tokens:  ['от', 'бойында', 'үн', 'келiншек', 'жүзiне']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_10.h5\n",
            "Main Epoch 11/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 63s 169ms/step - loss: 0.7512 - acc: 0.8123 - truncated_acc: 0.6951 - truncated_loss: 1.2193 - val_loss: 0.5226 - val_acc: 0.8684 - val_truncated_acc: 0.7862 - val_truncated_loss: 0.8490\n",
            "-\n",
            "Input tokens:   ['жауаптарын', 'бұрэын', 'дауа', 'МНқа', 'мүлЪде']\n",
            "Decoded tokens: ['жауан', 'бұраны', 'ауына', 'қарала', 'жалында']\n",
            "Target tokens:  ['жауаптарын', 'бұрын', 'дауа', 'Мұқа', 'мүлде']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_11.h5\n",
            "Main Epoch 12/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 64s 170ms/step - loss: 0.7249 - acc: 0.8251 - truncated_acc: 0.7160 - truncated_loss: 1.1770 - val_loss: 0.4215 - val_acc: 0.9161 - val_truncated_acc: 0.8637 - val_truncated_loss: 0.6849\n",
            "-\n",
            "Input tokens:   ['тегiс', 'дгеен', 'жағынад', 'күндестЁрден', 'кеЛттi']\n",
            "Decoded tokens: ['тегiсi', 'деген', 'жағында', 'күндестер', 'кеттi']\n",
            "Target tokens:  ['тегiс', 'деген', 'жағында', 'күндестерден', 'кеттi']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_12.h5\n",
            "Main Epoch 13/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 64s 172ms/step - loss: 0.6859 - acc: 0.8357 - truncated_acc: 0.7332 - truncated_loss: 1.1133 - val_loss: 0.3714 - val_acc: 0.9264 - val_truncated_acc: 0.8805 - val_truncated_loss: 0.6034\n",
            "-\n",
            "Input tokens:   ['сұраө', 'Бiлмеген', 'бiргШе', 'ыжм', 'мұвнына']\n",
            "Decoded tokens: ['сұра', 'Бiлмеген', 'бiрге', 'жыма', 'мұнынын']\n",
            "Target tokens:  ['сұрау', 'Бiлмеген', 'бiрге', 'жым', 'мұрнына']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_13.h5\n",
            "Main Epoch 14/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 63s 168ms/step - loss: 0.6823 - acc: 0.8376 - truncated_acc: 0.7363 - truncated_loss: 1.1078 - val_loss: 0.3582 - val_acc: 0.9290 - val_truncated_acc: 0.8846 - val_truncated_loss: 0.5820\n",
            "-\n",
            "Input tokens:   ['отырысқан', 'iблу', 'Мына', 'осГы', 'квiсiнi']\n",
            "Decoded tokens: ['отырасын', 'бiлу', 'Мына', 'осын', 'кесiнi']\n",
            "Target tokens:  ['отырысқан', 'бiлу', 'Мынау', 'осы', 'кiсiнi']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_14.h5\n",
            "Main Epoch 15/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 63s 169ms/step - loss: 0.6888 - acc: 0.8399 - truncated_acc: 0.7401 - truncated_loss: 1.1182 - val_loss: 0.3343 - val_acc: 0.9320 - val_truncated_acc: 0.8895 - val_truncated_loss: 0.5432\n",
            "-\n",
            "Input tokens:   ['базарды', 'отыштың', 'ба', 'жТзi', 'жүду']\n",
            "Decoded tokens: ['базарды', 'отының', 'бай', 'жазы', 'жүдеу']\n",
            "Target tokens:  ['базарды', 'Мотыштың', 'ба', 'жезi', 'жүдеу']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_15.h5\n",
            "Main Epoch 16/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 64s 171ms/step - loss: 0.6602 - acc: 0.8453 - truncated_acc: 0.7487 - truncated_loss: 1.0719 - val_loss: 0.3453 - val_acc: 0.9297 - val_truncated_acc: 0.8858 - val_truncated_loss: 0.5609\n",
            "-\n",
            "Input tokens:   ['барЙлық', 'бұғаПн', 'еең', 'қолмынан', 'Яiр']\n",
            "Decoded tokens: ['барлық', 'бұған', 'еңе', 'қолмынан', 'кiр']\n",
            "Target tokens:  ['барлық', 'бұған', 'елең', 'қолымнан', 'бiр']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_16.h5\n",
            "Main Epoch 17/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 65s 173ms/step - loss: 0.6542 - acc: 0.8484 - truncated_acc: 0.7538 - truncated_loss: 1.0621 - val_loss: 0.3457 - val_acc: 0.9323 - val_truncated_acc: 0.8900 - val_truncated_loss: 0.5617\n",
            "-\n",
            "Input tokens:   ['айтды', 'кеТтедi', 'Ертеңнiде', 'Ерұол', 'сияқты']\n",
            "Decoded tokens: ['айтын', 'кетедi', 'Ертеңiнде', 'Ерліл', 'сияқты']\n",
            "Target tokens:  ['айтады', 'кетедi', 'Ертеңiнде', 'Ербол', 'сияқты']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_17.h5\n",
            "Main Epoch 18/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 65s 172ms/step - loss: 0.6846 - acc: 0.8409 - truncated_acc: 0.7416 - truncated_loss: 1.1116 - val_loss: 0.3115 - val_acc: 0.9372 - val_truncated_acc: 0.8979 - val_truncated_loss: 0.5062\n",
            "-\n",
            "Input tokens:   ['Сәрсекеәнi', 'Тұранбай', 'екпiнде', 'Хшiп', 'айтыт']\n",
            "Decoded tokens: ['Сәрекесiн', 'Тұранбай', 'екiнде', 'шап', 'айтыт']\n",
            "Target tokens:  ['Сәрсекенi', 'Тұрғанбай', 'екпiндеп', 'iшiп', 'айтты']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_18.h5\n",
            "Main Epoch 19/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 63s 168ms/step - loss: 0.6500 - acc: 0.8478 - truncated_acc: 0.7529 - truncated_loss: 1.0552 - val_loss: 0.2794 - val_acc: 0.9401 - val_truncated_acc: 0.9027 - val_truncated_loss: 0.4541\n",
            "-\n",
            "Input tokens:   ['кп', 'бұғаПн', '–', 'көрiктео', 'еЛе']\n",
            "Decoded tokens: ['кеп', 'бұған', 'Ха', 'көрiкте', 'еле']\n",
            "Target tokens:  ['көп', 'бұған', '–', 'көрiктей', 'ере']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_19.h5\n",
            "Main Epoch 20/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 63s 168ms/step - loss: 0.6569 - acc: 0.8485 - truncated_acc: 0.7540 - truncated_loss: 1.0663 - val_loss: 0.2969 - val_acc: 0.9381 - val_truncated_acc: 0.8993 - val_truncated_loss: 0.4825\n",
            "-\n",
            "Input tokens:   ['ұзақ', 'аДтып', 'жосқ', 'құлаЗымен', '–']\n",
            "Decoded tokens: ['ұзақ', 'аттып', 'жосқа', 'құламына', 'қа']\n",
            "Target tokens:  ['ұзақ', 'айтып', 'жоқ', 'құлағымен', '–']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_20.h5\n",
            "Main Epoch 21/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 63s 169ms/step - loss: 0.6383 - acc: 0.8534 - truncated_acc: 0.7619 - truncated_loss: 1.0361 - val_loss: 0.3142 - val_acc: 0.9359 - val_truncated_acc: 0.8959 - val_truncated_loss: 0.5104\n",
            "-\n",
            "Input tokens:   ['iiңiз', 'Ендi', 'басшылЦары', 'деп', 'күштi']\n",
            "Decoded tokens: ['iрiңiз', 'Ендi', 'басшылары', 'деп', 'күштi']\n",
            "Target tokens:  ['iшiңiз', 'Ендi', 'басшылары', 'деп', 'күштi']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_21.h5\n",
            "Main Epoch 22/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 64s 171ms/step - loss: 0.6112 - acc: 0.8632 - truncated_acc: 0.7779 - truncated_loss: 0.9922 - val_loss: 0.3107 - val_acc: 0.9358 - val_truncated_acc: 0.8957 - val_truncated_loss: 0.5048\n",
            "-\n",
            "Input tokens:   ['аХртамын', 'үдние', 'қас', 'намыс', 'ет']\n",
            "Decoded tokens: ['артамын', 'үнде', 'қас', 'намыс', 'ет']\n",
            "Target tokens:  ['артамын', 'дүние', 'жас', 'намыс', 'ет']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_22.h5\n",
            "Main Epoch 23/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 65s 173ms/step - loss: 0.6387 - acc: 0.8538 - truncated_acc: 0.7626 - truncated_loss: 1.0371 - val_loss: 0.2958 - val_acc: 0.9412 - val_truncated_acc: 0.9045 - val_truncated_loss: 0.4807\n",
            "-\n",
            "Input tokens:   ['ТойсарЙы', 'болғанша', 'Буыны', 'қІлтарысына', 'утақытта']\n",
            "Decoded tokens: ['Тойсары', 'болғанша', 'Буыны', 'қалтарысына', 'тақытта']\n",
            "Target tokens:  ['Тойсары', 'болғанша', 'Буыны', 'қалтарысына', 'уақытта']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_23.h5\n",
            "Main Epoch 24/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 64s 170ms/step - loss: 0.6158 - acc: 0.8583 - truncated_acc: 0.7700 - truncated_loss: 0.9993 - val_loss: 0.3043 - val_acc: 0.9399 - val_truncated_acc: 0.9024 - val_truncated_loss: 0.4945\n",
            "-\n",
            "Input tokens:   ['көрпi', 'Чжұлып', 'еске', 'адам', 'Семейдiң']\n",
            "Decoded tokens: ['көрiп', 'жұлып', 'еске', 'адам', 'Семейдiң']\n",
            "Target tokens:  ['көрiп', 'жұлып', 'еске', 'адам', 'Семейдiң']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_24.h5\n",
            "Main Epoch 25/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 63s 167ms/step - loss: 0.6038 - acc: 0.8640 - truncated_acc: 0.7791 - truncated_loss: 0.9803 - val_loss: 0.2650 - val_acc: 0.9450 - val_truncated_acc: 0.9106 - val_truncated_loss: 0.4306\n",
            "-\n",
            "Input tokens:   ['оныс', 'Өжүр', 'ЕЁдi', 'аылп', 'бұбзастан']\n",
            "Decoded tokens: ['оныс', 'жүр', 'Едi', 'алып', 'бұзастан']\n",
            "Target tokens:  ['қоныс', 'жүр', 'Ендi', 'алып', 'бұзбастан']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_25.h5\n",
            "Main Epoch 26/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 63s 169ms/step - loss: 0.6446 - acc: 0.8519 - truncated_acc: 0.7595 - truncated_loss: 1.0467 - val_loss: 0.3871 - val_acc: 0.9254 - val_truncated_acc: 0.8788 - val_truncated_loss: 0.6283\n",
            "-\n",
            "Input tokens:   ['аттандырғаКн', 'Қыстың', 'Жол', 'тғаы', 'облғанмен']\n",
            "Decoded tokens: ['аттаны', 'Қасты', 'Жола', 'тағы', 'орылған']\n",
            "Target tokens:  ['аттандырған', 'Қыстың', 'Сол', 'тағы', 'болғанмен']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_26.h5\n",
            "Main Epoch 27/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 64s 170ms/step - loss: 0.6690 - acc: 0.8434 - truncated_acc: 0.7457 - truncated_loss: 1.0859 - val_loss: 0.3027 - val_acc: 0.9404 - val_truncated_acc: 0.9032 - val_truncated_loss: 0.4917\n",
            "-\n",
            "Input tokens:   ['лкен', 'Соға', 'Щалуан', 'Михйловты', 'бi']\n",
            "Decoded tokens: ['келен', 'Соға', 'алауын', 'Михайловты', 'бiл']\n",
            "Target tokens:  ['үлкен', 'Соған', 'алуан', 'Михайловты', 'бiз']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_27.h5\n",
            "Main Epoch 28/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 65s 173ms/step - loss: 0.6087 - acc: 0.8613 - truncated_acc: 0.7748 - truncated_loss: 0.9884 - val_loss: 0.3062 - val_acc: 0.9407 - val_truncated_acc: 0.9036 - val_truncated_loss: 0.4974\n",
            "-\n",
            "Input tokens:   ['жiгiттЭiң', 'келтiреiд', 'едi', 'Срыс', 'Нұрағным']\n",
            "Decoded tokens: ['жiгiттiң', 'келтiредi', 'едi', 'Сырыс', 'Нұрғаным']\n",
            "Target tokens:  ['жiгiттiң', 'келтiредi', 'едiм', 'орыс', 'Нұрғаным']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_28.h5\n",
            "Main Epoch 29/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 63s 169ms/step - loss: 0.5582 - acc: 0.8740 - truncated_acc: 0.7954 - truncated_loss: 0.9064 - val_loss: 0.2625 - val_acc: 0.9454 - val_truncated_acc: 0.9113 - val_truncated_loss: 0.4266\n",
            "-\n",
            "Input tokens:   ['және', 'хабраын', 'тсоып', 'бяғаланды', 'құйыншЮ']\n",
            "Decoded tokens: ['және', 'хабарын', 'тосып', 'бағаланды', 'құйыншы']\n",
            "Target tokens:  ['және', 'хабарын', 'тосып', 'бағаланды', 'құйынша']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_29.h5\n",
            "Main Epoch 30/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 63s 167ms/step - loss: 0.6148 - acc: 0.8596 - truncated_acc: 0.7720 - truncated_loss: 0.9981 - val_loss: 0.2433 - val_acc: 0.9459 - val_truncated_acc: 0.9121 - val_truncated_loss: 0.3953\n",
            "-\n",
            "Input tokens:   ['мейiрiмiнен', 'сәтде', 'жағы', 'вЕгений', 'әкеiснiң']\n",
            "Decoded tokens: ['мейiрiмiнен', 'сәтеде', 'жағы', 'Бегенн', 'әкесiнiң']\n",
            "Target tokens:  ['мейiрiмiнен', 'сәтте', 'жағы', 'Евгений', 'әкесiнiң']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_30.h5\n",
            "Main Epoch 31/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 63s 168ms/step - loss: 0.6348 - acc: 0.8542 - truncated_acc: 0.7632 - truncated_loss: 1.0307 - val_loss: 0.2853 - val_acc: 0.9459 - val_truncated_acc: 0.9120 - val_truncated_loss: 0.4635\n",
            "-\n",
            "Input tokens:   ['Үалгi', 'мысқыһдап', 'дастарқандарү', 'айтақн', 'iмздеп']\n",
            "Decoded tokens: ['Үлгi', 'мысқындап', 'дастар', 'айтқан', 'мiздеп']\n",
            "Target tokens:  ['Үлгi', 'мысқылдап', 'дастарқандары', 'айтқан', 'iздеп']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_31.h5\n",
            "Main Epoch 32/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 64s 170ms/step - loss: 0.6389 - acc: 0.8509 - truncated_acc: 0.7579 - truncated_loss: 1.0373 - val_loss: 0.2871 - val_acc: 0.9420 - val_truncated_acc: 0.9058 - val_truncated_loss: 0.4665\n",
            "-\n",
            "Input tokens:   ['Қкете', 'аңа', 'Абай', 'тұмантгып', 'кеткән']\n",
            "Decoded tokens: ['Қатер', 'аңа', 'Абай', 'тұмантып', 'кеткен']\n",
            "Target tokens:  ['кете', 'жаңа', 'Абай', 'тұмантып', 'кеткен']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_32.h5\n",
            "Main Epoch 33/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 64s 172ms/step - loss: 0.6010 - acc: 0.8638 - truncated_acc: 0.7788 - truncated_loss: 0.9755 - val_loss: 0.2806 - val_acc: 0.9443 - val_truncated_acc: 0.9095 - val_truncated_loss: 0.4559\n",
            "-\n",
            "Input tokens:   ['кее', 'қапты', 'соИ', 'ҚаҢзiрде', 'кеглен']\n",
            "Decoded tokens: ['кете', 'қапты', 'сол', 'Қазiрде', 'келген']\n",
            "Target tokens:  ['келе', 'қапты', 'сол', 'Қазiрде', 'келген']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_33.h5\n",
            "Main Epoch 34/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 63s 168ms/step - loss: 0.6333 - acc: 0.8500 - truncated_acc: 0.7564 - truncated_loss: 1.0283 - val_loss: 0.2815 - val_acc: 0.9480 - val_truncated_acc: 0.9156 - val_truncated_loss: 0.4574\n",
            "-\n",
            "Input tokens:   ['жсеiр', 'ерге', 'Сендi', 'жІгiт', 'жасыМ']\n",
            "Decoded tokens: ['жесiр', 'ерге', 'Сендi', 'жiгiт', 'жасын']\n",
            "Target tokens:  ['жесiр', 'жерге', 'ендi', 'жiгiт', 'жасыл']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_34.h5\n",
            "Main Epoch 35/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 63s 167ms/step - loss: 0.6318 - acc: 0.8582 - truncated_acc: 0.7698 - truncated_loss: 1.0256 - val_loss: 0.2715 - val_acc: 0.9468 - val_truncated_acc: 0.9136 - val_truncated_loss: 0.4412\n",
            "-\n",
            "Input tokens:   ['тбiрi', '–', 'еқапталынан', 'едi', 'iiшнде']\n",
            "Decoded tokens: ['тiбiр', 'қа', 'қаптала', 'едi', 'iшiнде']\n",
            "Target tokens:  ['бiрi', '–', 'қапталынан', 'едi', 'iшiнде']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_35.h5\n",
            "Main Epoch 36/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 63s 169ms/step - loss: 0.6302 - acc: 0.8526 - truncated_acc: 0.7607 - truncated_loss: 1.0233 - val_loss: 0.3178 - val_acc: 0.9412 - val_truncated_acc: 0.9045 - val_truncated_loss: 0.5164\n",
            "-\n",
            "Input tokens:   ['Иберсең', 'адуға', 'ңiледi', 'аңысн', 'адасқандарңыды']\n",
            "Decoded tokens: ['берсең', 'адауға', 'кiледi', 'аңысын', 'адасқандары']\n",
            "Target tokens:  ['берсең', 'дауға', 'үңiледi', 'аңысын', 'адасқандарыңды']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_36.h5\n",
            "Main Epoch 37/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 64s 170ms/step - loss: 0.6077 - acc: 0.8607 - truncated_acc: 0.7738 - truncated_loss: 0.9865 - val_loss: 0.3082 - val_acc: 0.9485 - val_truncated_acc: 0.9163 - val_truncated_loss: 0.5007\n",
            "-\n",
            "Input tokens:   ['өет', 'ТаҚтақ', 'шпашаң', 'Аәай', 'бiр']\n",
            "Decoded tokens: ['өте', 'Татақ', 'шапшаң', 'Аай', 'бiр']\n",
            "Target tokens:  ['өжет', 'Тастақ', 'шапшаң', 'Абай', 'бiр']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_37.h5\n",
            "Main Epoch 38/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 64s 170ms/step - loss: 0.5404 - acc: 0.8782 - truncated_acc: 0.8022 - truncated_loss: 0.8775 - val_loss: 0.3072 - val_acc: 0.9413 - val_truncated_acc: 0.9045 - val_truncated_loss: 0.4990\n",
            "-\n",
            "Input tokens:   ['еркекше', 'БаймағамбетҺтi', 'адамыд', 'Ал', 'тұ']\n",
            "Decoded tokens: ['еркелек', 'Байма', 'адамды', 'Ал', 'тұ']\n",
            "Target tokens:  ['еркекше', 'Баймағамбеттi', 'адамды', 'Ал', 'тұр']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_38.h5\n",
            "Main Epoch 39/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 63s 169ms/step - loss: 0.5819 - acc: 0.8677 - truncated_acc: 0.7851 - truncated_loss: 0.9448 - val_loss: 0.2562 - val_acc: 0.9463 - val_truncated_acc: 0.9127 - val_truncated_loss: 0.4163\n",
            "-\n",
            "Input tokens:   ['уедi', 'бұрын', 'қршыға', 'орындыққа', 'байқаФады']\n",
            "Decoded tokens: ['уедi', 'бұрын', 'қаршыға', 'орындыққа', 'байқадады']\n",
            "Target tokens:  ['едi', 'бұрын', 'қаршыға', 'орындыққа', 'байқалады']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_39.h5\n",
            "Main Epoch 40/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 63s 168ms/step - loss: 0.5935 - acc: 0.8659 - truncated_acc: 0.7823 - truncated_loss: 0.9632 - val_loss: 0.2558 - val_acc: 0.9518 - val_truncated_acc: 0.9216 - val_truncated_loss: 0.4153\n",
            "-\n",
            "Input tokens:   ['үрегiне', 'тартысынН', 'гжелдей', 'iш', 'құшақтаса']\n",
            "Decoded tokens: ['үрегiне', 'тартысына', 'желдей', 'iш', 'құшақтаса']\n",
            "Target tokens:  ['жүрегiне', 'тартысына', 'желдей', 'iш', 'құшақтаса']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_40.h5\n",
            "Main Epoch 41/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 63s 167ms/step - loss: 0.6615 - acc: 0.8431 - truncated_acc: 0.7452 - truncated_loss: 1.0738 - val_loss: 0.2472 - val_acc: 0.9569 - val_truncated_acc: 0.9299 - val_truncated_loss: 0.4016\n",
            "-\n",
            "Input tokens:   ['абарға', 'Тфу', 'шығрп', 'халыХ', 'кедзескенде']\n",
            "Decoded tokens: ['абарға', 'Тау', 'шығыр', 'халын', 'кездескенде']\n",
            "Target tokens:  ['хабарға', 'Тьфу', 'шығып', 'халық', 'кездескенде']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_41.h5\n",
            "Main Epoch 42/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 62s 165ms/step - loss: 0.6145 - acc: 0.8554 - truncated_acc: 0.7652 - truncated_loss: 0.9979 - val_loss: 0.2792 - val_acc: 0.9544 - val_truncated_acc: 0.9259 - val_truncated_loss: 0.4537\n",
            "-\n",
            "Input tokens:   ['кеттi', '–', 'Шәкү', 'жӨп', 'қойбан']\n",
            "Decoded tokens: ['кеттi', '–', 'Шүк', 'жеп', 'қойбан']\n",
            "Target tokens:  ['кеттi', '–', 'Шәке', 'жап', 'қойған']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_42.h5\n",
            "Main Epoch 43/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 61s 164ms/step - loss: 0.6615 - acc: 0.8452 - truncated_acc: 0.7486 - truncated_loss: 1.0738 - val_loss: 0.3118 - val_acc: 0.9498 - val_truncated_acc: 0.9184 - val_truncated_loss: 0.5066\n",
            "-\n",
            "Input tokens:   ['Қысна', 'маынау', 'көлденеЖңдеп', 'үй', 'мiндет']\n",
            "Decoded tokens: ['Қысына', 'маныаа', 'көлденеңде', 'үй', 'мiндет']\n",
            "Target tokens:  ['Қысқа', 'мынау', 'көлденеңдеп', 'үй', 'мiндет']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_43.h5\n",
            "Main Epoch 44/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 61s 162ms/step - loss: 0.6320 - acc: 0.8505 - truncated_acc: 0.7572 - truncated_loss: 1.0262 - val_loss: 0.2476 - val_acc: 0.9579 - val_truncated_acc: 0.9316 - val_truncated_loss: 0.4023\n",
            "-\n",
            "Input tokens:   ['ешйiн', 'анқы', 'iшiньен', 'ЖыЧ', 'түскен']\n",
            "Decoded tokens: ['ешiн', 'анық', 'iшiнен', 'Жыр', 'түскен']\n",
            "Target tokens:  ['шейiн', 'анық', 'iшiнен', 'Жып', 'түскен']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_44.h5\n",
            "Main Epoch 45/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 61s 161ms/step - loss: 0.6308 - acc: 0.8539 - truncated_acc: 0.7628 - truncated_loss: 1.0238 - val_loss: 0.2369 - val_acc: 0.9557 - val_truncated_acc: 0.9281 - val_truncated_loss: 0.3849\n",
            "-\n",
            "Input tokens:   ['ашт', 'ықп', 'жәЪне', 'шлқытып', 'көрпенi']\n",
            "Decoded tokens: ['ашты', 'ықпа', 'жәні', 'шылқытып', 'көрпенi']\n",
            "Target tokens:  ['ашты', 'қып', 'және', 'шалқытып', 'көрпенiң']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_45.h5\n",
            "Main Epoch 46/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 61s 163ms/step - loss: 0.6327 - acc: 0.8476 - truncated_acc: 0.7525 - truncated_loss: 1.0272 - val_loss: 0.2245 - val_acc: 0.9562 - val_truncated_acc: 0.9289 - val_truncated_loss: 0.3648\n",
            "-\n",
            "Input tokens:   ['ер', 'Дәяркембайдың', 'ме', 'тапҢпады', 'саамағаны']\n",
            "Decoded tokens: ['ер', 'Дәркембайдың', 'ме', 'таппады', 'самағаны']\n",
            "Target tokens:  ['ер', 'Дәркембайдың', 'мен', 'таппады', 'санамағаны']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_46.h5\n",
            "Main Epoch 47/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 62s 165ms/step - loss: 0.6752 - acc: 0.8385 - truncated_acc: 0.7377 - truncated_loss: 1.0962 - val_loss: 0.2817 - val_acc: 0.9568 - val_truncated_acc: 0.9299 - val_truncated_loss: 0.4577\n",
            "-\n",
            "Input tokens:   ['Төрнде', 'менеі', 'Эмынандай', 'салдырып', 'Абпай']\n",
            "Decoded tokens: ['Төрінде', 'менеді', 'мынындай', 'салдырып', 'Абпай']\n",
            "Target tokens:  ['Төрiнде', 'менен', 'мынандай', 'салдырып', 'Абай']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_47.h5\n",
            "Main Epoch 48/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 61s 162ms/step - loss: 0.6432 - acc: 0.8473 - truncated_acc: 0.7521 - truncated_loss: 1.0442 - val_loss: 0.2552 - val_acc: 0.9588 - val_truncated_acc: 0.9331 - val_truncated_loss: 0.4146\n",
            "-\n",
            "Input tokens:   ['таып', 'сол', 'аралдЪй', 'сыңЖыл', 'БӨл']\n",
            "Decoded tokens: ['татып', 'сол', 'аралдай', 'сыңыл', 'Бұл']\n",
            "Target tokens:  ['тамып', 'сол', 'аралдай', 'сыңқыл', 'Бұл']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_48.h5\n",
            "Main Epoch 49/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 61s 162ms/step - loss: 0.6744 - acc: 0.8385 - truncated_acc: 0.7377 - truncated_loss: 1.0950 - val_loss: 0.3000 - val_acc: 0.9542 - val_truncated_acc: 0.9255 - val_truncated_loss: 0.4874\n",
            "-\n",
            "Input tokens:   ['әуоие', 'Һаса', 'күшiФне', 'тер', 'қырқыжлың']\n",
            "Decoded tokens: ['әуеие', 'аса', 'күшiне', 'тер', 'қырқылың']\n",
            "Target tokens:  ['әулие', 'аса', 'күшiне', 'өтер', 'қырқылжың']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_49.h5\n",
            "Main Epoch 50/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 61s 163ms/step - loss: 0.6370 - acc: 0.8502 - truncated_acc: 0.7567 - truncated_loss: 1.0341 - val_loss: 0.2567 - val_acc: 0.9552 - val_truncated_acc: 0.9273 - val_truncated_loss: 0.4171\n",
            "-\n",
            "Input tokens:   ['кнi', 'Ебол', 'Жоңбағанның', 'соңғы', 'жаауп']\n",
            "Decoded tokens: ['кенi', 'бол', 'Жоңағанның', 'соңғы', 'жауап']\n",
            "Target tokens:  ['күнi', 'Ербол', 'оңбағанның', 'соңғы', 'жауап']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_50.h5\n",
            "Main Epoch 51/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 62s 165ms/step - loss: 0.6601 - acc: 0.8410 - truncated_acc: 0.7418 - truncated_loss: 1.0715 - val_loss: 0.2336 - val_acc: 0.9584 - val_truncated_acc: 0.9325 - val_truncated_loss: 0.3796\n",
            "-\n",
            "Input tokens:   ['деп', 'ағам', 'үленнен', 'баяйланып', 'ос']\n",
            "Decoded tokens: ['деп', 'ағам', 'үленнен', 'байланып', 'ос']\n",
            "Target tokens:  ['деп', 'ағам', 'Шүленнен', 'байланып', 'ос']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_51.h5\n",
            "Main Epoch 52/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 61s 163ms/step - loss: 0.6374 - acc: 0.8492 - truncated_acc: 0.7552 - truncated_loss: 1.0346 - val_loss: 0.2491 - val_acc: 0.9593 - val_truncated_acc: 0.9339 - val_truncated_loss: 0.4045\n",
            "-\n",
            "Input tokens:   ['деiд', 'патшавыққа', 'домбыамды', 'атқанын', 'алып']\n",
            "Decoded tokens: ['дедi', 'патшалыққа', 'домбымады', 'атқанын', 'алып']\n",
            "Target tokens:  ['дедi', 'патшалыққа', 'домбырамды', 'айтқанын', 'алып']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_52.h5\n",
            "Main Epoch 53/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 61s 162ms/step - loss: 0.6575 - acc: 0.8392 - truncated_acc: 0.7389 - truncated_loss: 1.0676 - val_loss: 0.2869 - val_acc: 0.9560 - val_truncated_acc: 0.9285 - val_truncated_loss: 0.4661\n",
            "-\n",
            "Input tokens:   ['де', 'жанжал', 'ншi', 'Байкөше', 'сөiзн']\n",
            "Decoded tokens: ['де', 'жанжал', 'нешi', 'Байкөше', 'сөзiн']\n",
            "Target tokens:  ['де', 'жанжал', 'Әншi', 'Байкөкше', 'сөзiн']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_53.h5\n",
            "Main Epoch 54/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 61s 162ms/step - loss: 0.6521 - acc: 0.8405 - truncated_acc: 0.7410 - truncated_loss: 1.0590 - val_loss: 0.2205 - val_acc: 0.9603 - val_truncated_acc: 0.9356 - val_truncated_loss: 0.3583\n",
            "-\n",
            "Input tokens:   ['мүмiн', 'узеiнен', 'Қаракесек', 'да', 'еку']\n",
            "Decoded tokens: ['мүмiн', 'узенiне', 'Қаракесек', 'да', 'екеу']\n",
            "Target tokens:  ['мүмкiн', 'уезiнен', 'Қаракесек', 'да', 'кеу']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_54.h5\n",
            "Main Epoch 55/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 62s 164ms/step - loss: 0.6504 - acc: 0.8459 - truncated_acc: 0.7498 - truncated_loss: 1.0561 - val_loss: 0.2472 - val_acc: 0.9545 - val_truncated_acc: 0.9261 - val_truncated_loss: 0.4016\n",
            "-\n",
            "Input tokens:   ['Рбасып', 'енiд', 'екеД', 'молдағы', 'жүргiззiп']\n",
            "Decoded tokens: ['басып', 'ендi', 'екен', 'молдағы', 'жүргiзiп']\n",
            "Target tokens:  ['басып', 'ендi', 'екен', 'молдаға', 'жүргiзiп']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_55.h5\n",
            "Main Epoch 56/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 61s 164ms/step - loss: 0.6817 - acc: 0.8343 - truncated_acc: 0.7309 - truncated_loss: 1.1068 - val_loss: 0.2371 - val_acc: 0.9560 - val_truncated_acc: 0.9284 - val_truncated_loss: 0.3853\n",
            "-\n",
            "Input tokens:   ['кедлi', 'ендгii', 'ыШза', 'енiд', 'болмйады']\n",
            "Decoded tokens: ['келдi', 'ендiгi', 'Шыза', 'ендi', 'болмайды']\n",
            "Target tokens:  ['келдi', 'ендiгi', 'ырза', 'ендi', 'болмайды']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_56.h5\n",
            "Main Epoch 57/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 62s 166ms/step - loss: 0.6725 - acc: 0.8327 - truncated_acc: 0.7282 - truncated_loss: 1.0920 - val_loss: 0.2722 - val_acc: 0.9557 - val_truncated_acc: 0.9281 - val_truncated_loss: 0.4421\n",
            "-\n",
            "Input tokens:   ['оқ', 'болаыд', 'оқымайды', 'қолыда', 'сЦл']\n",
            "Decoded tokens: ['оқ', 'болады', 'оқымайды', 'қолыда', 'сал']\n",
            "Target tokens:  ['жоқ', 'болады', 'оқымайды', 'қолында', 'сол']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_57.h5\n",
            "Main Epoch 58/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 61s 163ms/step - loss: 0.5704 - acc: 0.8669 - truncated_acc: 0.7838 - truncated_loss: 0.9262 - val_loss: 0.2228 - val_acc: 0.9596 - val_truncated_acc: 0.9343 - val_truncated_loss: 0.3620\n",
            "-\n",
            "Input tokens:   ['юұрау', 'ағайҰн', 'ау', 'да', 'отырған']\n",
            "Decoded tokens: ['жұрау', 'ағайын', 'ау', 'да', 'отырған']\n",
            "Target tokens:  ['құрау', 'ағайын', 'ау', 'да', 'отырған']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_58.h5\n",
            "Main Epoch 59/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 62s 165ms/step - loss: 0.6071 - acc: 0.8562 - truncated_acc: 0.7664 - truncated_loss: 0.9856 - val_loss: 0.2670 - val_acc: 0.9552 - val_truncated_acc: 0.9271 - val_truncated_loss: 0.4336\n",
            "-\n",
            "Input tokens:   ['Эедi', 'Ясалатын', 'разбоІйник', 'көп', 'ддеi']\n",
            "Decoded tokens: ['ердi', 'салатын', 'разбок', 'көп', 'дедi']\n",
            "Target tokens:  ['едi', 'салатын', 'разбойник', 'көп', 'дедi']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_59.h5\n",
            "Main Epoch 60/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 62s 165ms/step - loss: 0.6428 - acc: 0.8458 - truncated_acc: 0.7496 - truncated_loss: 1.0433 - val_loss: 0.2560 - val_acc: 0.9555 - val_truncated_acc: 0.9276 - val_truncated_loss: 0.4158\n",
            "-\n",
            "Input tokens:   ['жақынъы', 'Қиалаға', 'ұзатЬлды', 'әзiлдеп', 'гдостық']\n",
            "Decoded tokens: ['жақыны', 'Қилаға', 'ұзаталды', 'әзiлдеп', 'достық']\n",
            "Target tokens:  ['жақыны', 'Қалаға', 'ұзатылды', 'әзiлдеп', 'достық']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_60.h5\n",
            "Main Epoch 61/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 61s 164ms/step - loss: 0.6383 - acc: 0.8459 - truncated_acc: 0.7496 - truncated_loss: 1.0366 - val_loss: 0.2781 - val_acc: 0.9565 - val_truncated_acc: 0.9294 - val_truncated_loss: 0.4519\n",
            "-\n",
            "Input tokens:   ['–', 'сайғаРн', 'емн', 'шай', 'бРұрын']\n",
            "Decoded tokens: ['–', 'сайған', 'емін', 'шай', 'бұрын']\n",
            "Target tokens:  ['–', 'сайған', 'мен', 'шай', 'бұрын']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_61.h5\n",
            "Main Epoch 62/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 62s 164ms/step - loss: 0.5737 - acc: 0.8638 - truncated_acc: 0.7787 - truncated_loss: 0.9312 - val_loss: 0.2529 - val_acc: 0.9575 - val_truncated_acc: 0.9310 - val_truncated_loss: 0.4109\n",
            "-\n",
            "Input tokens:   ['маңайЙы', 'оыс', '–', 'да', 'сойысл']\n",
            "Decoded tokens: ['маңайы', 'отыс', '–', 'да', 'сойыс']\n",
            "Target tokens:  ['маңайды', 'осы', '–', 'да', 'сойыл']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_62.h5\n",
            "Main Epoch 63/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 62s 165ms/step - loss: 0.6066 - acc: 0.8605 - truncated_acc: 0.7735 - truncated_loss: 0.9848 - val_loss: 0.2504 - val_acc: 0.9556 - val_truncated_acc: 0.9279 - val_truncated_loss: 0.4069\n",
            "-\n",
            "Input tokens:   ['таып', 'Мен', 'не', 'сiлте', 'жұрттың']\n",
            "Decoded tokens: ['тарып', 'Мен', 'не', 'сiлте', 'жұрттың']\n",
            "Target tokens:  ['танып', 'Мен', 'не', 'сiлтеп', 'жұрттың']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_63.h5\n",
            "Main Epoch 64/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 62s 166ms/step - loss: 0.5794 - acc: 0.8623 - truncated_acc: 0.7764 - truncated_loss: 0.9407 - val_loss: 0.2436 - val_acc: 0.9554 - val_truncated_acc: 0.9275 - val_truncated_loss: 0.3958\n",
            "-\n",
            "Input tokens:   ['бАай', 'тек', 'ауылдара', 'отырғае', 'оырып']\n",
            "Decoded tokens: ['бай', 'тек', 'ауылдар', 'отырған', 'отырып']\n",
            "Target tokens:  ['Абай', 'тек', 'ауылдарға', 'отырған', 'отырып']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_64.h5\n",
            "Main Epoch 65/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 62s 166ms/step - loss: 0.5916 - acc: 0.8592 - truncated_acc: 0.7714 - truncated_loss: 0.9605 - val_loss: 0.2468 - val_acc: 0.9593 - val_truncated_acc: 0.9339 - val_truncated_loss: 0.4009\n",
            "-\n",
            "Input tokens:   ['ме', 'Кеiйн', 'Йкруг', 'шшее', 'бағалад']\n",
            "Decoded tokens: ['ме', 'Кейiн', 'каруы', 'шеше', 'бағалады']\n",
            "Target tokens:  ['ме', 'Кейiн', 'округ', 'шеше', 'бағалады']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_65.h5\n",
            "Main Epoch 66/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 62s 164ms/step - loss: 0.5591 - acc: 0.8677 - truncated_acc: 0.7852 - truncated_loss: 0.9077 - val_loss: 0.2039 - val_acc: 0.9612 - val_truncated_acc: 0.9370 - val_truncated_loss: 0.3313\n",
            "-\n",
            "Input tokens:   ['тоспаңадр', 'үалатын', 'тұрмын', 'Абылғаызсы', 'СүйАтiп']\n",
            "Decoded tokens: ['тоспаңдар', 'алатын', 'тұрмын', 'Абылғазысы', 'Сүйтiп']\n",
            "Target tokens:  ['тоспаңдар', 'салатын', 'тұрмын', 'Абылғазысы', 'Сүйтiп']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_66.h5\n",
            "Main Epoch 67/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 62s 165ms/step - loss: 0.6152 - acc: 0.8550 - truncated_acc: 0.7645 - truncated_loss: 0.9990 - val_loss: 0.2383 - val_acc: 0.9553 - val_truncated_acc: 0.9274 - val_truncated_loss: 0.3872\n",
            "-\n",
            "Input tokens:   ['Абанй', 'ат', 'еiд', 'щоң', 'үзетiНн']\n",
            "Decoded tokens: ['Абай', 'ат', 'едi', 'жоң', 'үзетiн']\n",
            "Target tokens:  ['Абай', 'ат', 'едi', 'соң', 'үзетiн']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_67.h5\n",
            "Main Epoch 68/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 62s 165ms/step - loss: 0.6317 - acc: 0.8515 - truncated_acc: 0.7589 - truncated_loss: 1.0258 - val_loss: 0.2445 - val_acc: 0.9586 - val_truncated_acc: 0.9328 - val_truncated_loss: 0.3972\n",
            "-\n",
            "Input tokens:   ['түсп', 'абран', 'елеуiсз', 'Айтцан', 'дедбi']\n",
            "Decoded tokens: ['түсіп', 'баран', 'елеусiз', 'Айтан', 'дедi']\n",
            "Target tokens:  ['түсiп', 'баран', 'елеусiз', 'Айтқан', 'дедi']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_68.h5\n",
            "Main Epoch 69/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 62s 164ms/step - loss: 0.6171 - acc: 0.8545 - truncated_acc: 0.7637 - truncated_loss: 1.0016 - val_loss: 0.2515 - val_acc: 0.9571 - val_truncated_acc: 0.9302 - val_truncated_loss: 0.4085\n",
            "-\n",
            "Input tokens:   ['дйемiн', 'едп', 'қайратты', '–', 'бра']\n",
            "Decoded tokens: ['деймiн', 'деп', 'қайратты', '–', 'бара']\n",
            "Target tokens:  ['деймiн', 'деп', 'қайратты', '–', 'бар']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_69.h5\n",
            "Main Epoch 70/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 61s 163ms/step - loss: 0.6633 - acc: 0.8417 - truncated_acc: 0.7430 - truncated_loss: 1.0770 - val_loss: 0.2385 - val_acc: 0.9602 - val_truncated_acc: 0.9352 - val_truncated_loss: 0.3875\n",
            "-\n",
            "Input tokens:   ['жiiттер', 'ең', 'МҺихайлов', '–', 'шәугiШ']\n",
            "Decoded tokens: ['жiгiттер', 'ең', 'Михайлов', '–', 'шәугi']\n",
            "Target tokens:  ['жiгiттер', 'кең', 'Михайлов', '–', 'шәугiм']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_70.h5\n",
            "Main Epoch 71/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 61s 161ms/step - loss: 0.6286 - acc: 0.8486 - truncated_acc: 0.7542 - truncated_loss: 1.0207 - val_loss: 0.2395 - val_acc: 0.9576 - val_truncated_acc: 0.9311 - val_truncated_loss: 0.3891\n",
            "-\n",
            "Input tokens:   ['да', 'үйдуен', 'отн', 'еһкен', 'дегедi']\n",
            "Decoded tokens: ['да', 'үйден', 'тон', 'екен', 'дегендi']\n",
            "Target tokens:  ['да', 'үйден', 'отын', 'еткен', 'дегендi']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_71.h5\n",
            "Main Epoch 72/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 62s 165ms/step - loss: 0.6251 - acc: 0.8513 - truncated_acc: 0.7585 - truncated_loss: 1.0148 - val_loss: 0.2515 - val_acc: 0.9604 - val_truncated_acc: 0.9357 - val_truncated_loss: 0.4083\n",
            "-\n",
            "Input tokens:   ['шелегiмен', 'неше', 'қалң', 'кiснiң', 'күтуег']\n",
            "Decoded tokens: ['шелегiмен', 'неше', 'қалаң', 'кiсiнiң', 'күтуге']\n",
            "Target tokens:  ['шелегiмен', 'неше', 'қалың', 'кiсiнiң', 'күтуге']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_72.h5\n",
            "Main Epoch 73/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 61s 164ms/step - loss: 0.6099 - acc: 0.8578 - truncated_acc: 0.7690 - truncated_loss: 0.9902 - val_loss: 0.2175 - val_acc: 0.9611 - val_truncated_acc: 0.9368 - val_truncated_loss: 0.3534\n",
            "-\n",
            "Input tokens:   ['та', 'қысылһп', 'сәте', 'бл', 'үрсе']\n",
            "Decoded tokens: ['та', 'қысылып', 'сәте', 'бал', 'үрсе']\n",
            "Target tokens:  ['та', 'қысылып', 'сәтте', 'бұл', 'жүрсе']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_73.h5\n",
            "Main Epoch 74/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 61s 163ms/step - loss: 0.6096 - acc: 0.8563 - truncated_acc: 0.7666 - truncated_loss: 0.9897 - val_loss: 0.2041 - val_acc: 0.9641 - val_truncated_acc: 0.9416 - val_truncated_loss: 0.3317\n",
            "-\n",
            "Input tokens:   ['еiд', 'шаң', 'ысқа', 'осл', 'көшiiп']\n",
            "Decoded tokens: ['едi', 'шаң', 'ысқа', 'сол', 'көшiлiп']\n",
            "Target tokens:  ['едi', 'шаң', 'қысқа', 'сол', 'көшiрiп']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_74.h5\n",
            "Main Epoch 75/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 61s 162ms/step - loss: 0.6150 - acc: 0.8530 - truncated_acc: 0.7613 - truncated_loss: 0.9985 - val_loss: 0.2589 - val_acc: 0.9614 - val_truncated_acc: 0.9373 - val_truncated_loss: 0.4206\n",
            "-\n",
            "Input tokens:   ['Мезн', 'пәтерлернiе', 'тоырғандай', 'еъкендей', 'айты']\n",
            "Decoded tokens: ['Мезiн', 'пәтерлерiне', 'отырғандай', 'екендей', 'айты']\n",
            "Target tokens:  ['Мен', 'пәтерлерiне', 'отырғандай', 'еткендей', 'айтты']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_75.h5\n",
            "Main Epoch 76/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 61s 162ms/step - loss: 0.6300 - acc: 0.8491 - truncated_acc: 0.7550 - truncated_loss: 1.0229 - val_loss: 0.2653 - val_acc: 0.9609 - val_truncated_acc: 0.9365 - val_truncated_loss: 0.4311\n",
            "-\n",
            "Input tokens:   ['бболғанын', 'бастаүан', 'шақшы', 'иненi', 'зындаЭ']\n",
            "Decoded tokens: ['болғанын', 'бастанан', 'шақшы', 'иненi', 'зындай']\n",
            "Target tokens:  ['болғанын', 'бастаған', 'шашы', 'иненi', 'зындан']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_76.h5\n",
            "Main Epoch 77/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 62s 164ms/step - loss: 0.6230 - acc: 0.8541 - truncated_acc: 0.7630 - truncated_loss: 1.0114 - val_loss: 0.2283 - val_acc: 0.9616 - val_truncated_acc: 0.9376 - val_truncated_loss: 0.3710\n",
            "-\n",
            "Input tokens:   ['шошды', 'мған', 'бiтiрiп', 'шыымын', 'сөйлейдi']\n",
            "Decoded tokens: ['шошды', 'маған', 'бiтiрiп', 'шығымын', 'сөйлейдi']\n",
            "Target tokens:  ['шомды', 'маған', 'бiтiрiп', 'шырымын', 'сөйлейдi']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_77.h5\n",
            "Main Epoch 78/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 61s 163ms/step - loss: 0.6320 - acc: 0.8517 - truncated_acc: 0.7591 - truncated_loss: 1.0257 - val_loss: 0.2660 - val_acc: 0.9597 - val_truncated_acc: 0.9346 - val_truncated_loss: 0.4320\n",
            "-\n",
            "Input tokens:   ['қйата', 'iке', 'үңiълдi', 'лiес', 'бар']\n",
            "Decoded tokens: ['қайта', 'iшке', 'үңiлдi', 'iлес', 'бар']\n",
            "Target tokens:  ['қайта', 'iске', 'үңiлдi', 'iлес', 'бар']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_78.h5\n",
            "Main Epoch 79/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 61s 162ms/step - loss: 0.5939 - acc: 0.8595 - truncated_acc: 0.7719 - truncated_loss: 0.9644 - val_loss: 0.2235 - val_acc: 0.9639 - val_truncated_acc: 0.9414 - val_truncated_loss: 0.3631\n",
            "-\n",
            "Input tokens:   ['отырған', 'ежтiстi', 'йелдер', 'отырған', 'СаАптама']\n",
            "Decoded tokens: ['отырған', 'жетiстi', 'елдер', 'отырған', 'Саптама']\n",
            "Target tokens:  ['отырған', 'жетiстi', 'әйелдер', 'отырған', 'Саптама']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_79.h5\n",
            "Main Epoch 80/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 61s 162ms/step - loss: 0.6427 - acc: 0.8474 - truncated_acc: 0.7523 - truncated_loss: 1.0432 - val_loss: 0.2626 - val_acc: 0.9580 - val_truncated_acc: 0.9317 - val_truncated_loss: 0.4265\n",
            "-\n",
            "Input tokens:   ['сырттан', 'ұмытпғаандай', 'жаҺы', 'алалары', 'сзөге']\n",
            "Decoded tokens: ['сырттан', 'ұмытпағандай', 'жалы', 'алалары', 'сөзге']\n",
            "Target tokens:  ['сырттан', 'ұмытпағандай', 'жасы', 'балалары', 'сөзге']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_80.h5\n",
            "Main Epoch 81/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 61s 163ms/step - loss: 0.6122 - acc: 0.8542 - truncated_acc: 0.7633 - truncated_loss: 0.9942 - val_loss: 0.2258 - val_acc: 0.9611 - val_truncated_acc: 0.9367 - val_truncated_loss: 0.3668\n",
            "-\n",
            "Input tokens:   ['бүгiпнгi', 'жолы', 'алаям', 'жөнiһмдi', 'алғц']\n",
            "Decoded tokens: ['бүгiнгi', 'жолы', 'алам', 'жөнiмдi', 'алғы']\n",
            "Target tokens:  ['бүгiнгi', 'жолы', 'алам', 'жөнiмдi', 'алға']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_81.h5\n",
            "Main Epoch 82/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 61s 164ms/step - loss: 0.6298 - acc: 0.8519 - truncated_acc: 0.7596 - truncated_loss: 1.0226 - val_loss: 0.2316 - val_acc: 0.9619 - val_truncated_acc: 0.9381 - val_truncated_loss: 0.3764\n",
            "-\n",
            "Input tokens:   ['иықпне', 'ұйыЦп', 'үбгiн', 'барлй', 'түсп']\n",
            "Decoded tokens: ['иықпен', 'ұйып', 'бүгiн', 'барлай', 'түспе']\n",
            "Target tokens:  ['иықпен', 'ұйып', 'бүгiн', 'барлай', 'түсiп']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_82.h5\n",
            "Main Epoch 83/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 61s 161ms/step - loss: 0.6182 - acc: 0.8535 - truncated_acc: 0.7621 - truncated_loss: 1.0038 - val_loss: 0.2144 - val_acc: 0.9596 - val_truncated_acc: 0.9344 - val_truncated_loss: 0.3484\n",
            "-\n",
            "Input tokens:   ['Қшипа', 'ат', 'ас', 'iшiндйе', '–']\n",
            "Decoded tokens: ['Қипа', 'ат', 'ас', 'iшiндей', '–']\n",
            "Target tokens:  ['шипа', 'хат', 'ас', 'iшiнде', '–']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_83.h5\n",
            "Main Epoch 84/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 61s 162ms/step - loss: 0.5988 - acc: 0.8594 - truncated_acc: 0.7716 - truncated_loss: 0.9723 - val_loss: 0.2524 - val_acc: 0.9568 - val_truncated_acc: 0.9299 - val_truncated_loss: 0.4101\n",
            "-\n",
            "Input tokens:   ['уқаалп', 'кiсiлерн', 'ол', 'жлеiсiн', 'дүбiлретiп']\n",
            "Decoded tokens: ['қалалып', 'кiсiлерiн', 'ол', 'желiсiн', 'дүбiрлетiп']\n",
            "Target tokens:  ['уқалап', 'кiсiлерiн', 'ол', 'желiсiн', 'дүбiрлетiп']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_84.h5\n",
            "Main Epoch 85/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 62s 166ms/step - loss: 0.6185 - acc: 0.8564 - truncated_acc: 0.7667 - truncated_loss: 1.0041 - val_loss: 0.2386 - val_acc: 0.9589 - val_truncated_acc: 0.9333 - val_truncated_loss: 0.3877\n",
            "-\n",
            "Input tokens:   ['Пбұзықтары–', 'Жалғыздықтан', 'Қаладуа', 'СоныЬң', 'бойынан']\n",
            "Decoded tokens: ['бұзықтары–', 'Жалғыздықтан', 'Қалада', 'Соның', 'бойынан']\n",
            "Target tokens:  ['бұзықтары–', 'Жалғыздықтан', 'Қалада', 'Соның', 'бойынан']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_85.h5\n",
            "Main Epoch 86/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 62s 164ms/step - loss: 0.6310 - acc: 0.8480 - truncated_acc: 0.7531 - truncated_loss: 1.0246 - val_loss: 0.2418 - val_acc: 0.9588 - val_truncated_acc: 0.9331 - val_truncated_loss: 0.3929\n",
            "-\n",
            "Input tokens:   ['аслысымен', 'шауаларын', 'БаймағамбеттiЁң', '–', 'отырағндай']\n",
            "Decoded tokens: ['салысымен', 'шауаларын', 'Баймағамбеттiң', '–', 'отырғандай']\n",
            "Target tokens:  ['салысымен', 'шаруаларын', 'Баймағамбеттiң', '–', 'отырғандай']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_86.h5\n",
            "Main Epoch 87/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 61s 164ms/step - loss: 0.5871 - acc: 0.8647 - truncated_acc: 0.7803 - truncated_loss: 0.9529 - val_loss: 0.2632 - val_acc: 0.9584 - val_truncated_acc: 0.9323 - val_truncated_loss: 0.4275\n",
            "-\n",
            "Input tokens:   ['iшРiнде', 'соғЬып', 'шыӘлдырай', 'зңшы', 'Үлкіндеу']\n",
            "Decoded tokens: ['iшiнде', 'соғып', 'шылдыр', 'заңшы', 'Үлкіндеу']\n",
            "Target tokens:  ['iшiнде', 'соғып', 'шылдырай', 'аңшы', 'Үлкендеу']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_87.h5\n",
            "Main Epoch 88/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 61s 162ms/step - loss: 0.5916 - acc: 0.8617 - truncated_acc: 0.7754 - truncated_loss: 0.9605 - val_loss: 0.2218 - val_acc: 0.9607 - val_truncated_acc: 0.9361 - val_truncated_loss: 0.3603\n",
            "-\n",
            "Input tokens:   ['Баймағамбет', 'адыр', 'мофйын', 'сездi', 'сезБiмтал']\n",
            "Decoded tokens: ['Баймағамбет', 'адыр', 'мойын', 'сездi', 'сезiмтал']\n",
            "Target tokens:  ['Баймағамбет', 'адыр', 'мойын', 'сездi', 'сезiмтал']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_88.h5\n",
            "Main Epoch 89/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 61s 163ms/step - loss: 0.6109 - acc: 0.8558 - truncated_acc: 0.7658 - truncated_loss: 0.9917 - val_loss: 0.2364 - val_acc: 0.9606 - val_truncated_acc: 0.9360 - val_truncated_loss: 0.3841\n",
            "-\n",
            "Input tokens:   ['Окесек', 'ҚатБты', 'Сүгыр', 'Тыса', 'Бафмағамбетке']\n",
            "Decoded tokens: ['кеске', 'Қатты', 'Салыс', 'Тыса', 'Балмағамбетке']\n",
            "Target tokens:  ['кесек', 'Қатты', 'Сүгiр', 'Тыста', 'Баймағамбетке']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_89.h5\n",
            "Main Epoch 90/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 62s 166ms/step - loss: 0.5670 - acc: 0.8694 - truncated_acc: 0.7879 - truncated_loss: 0.9207 - val_loss: 0.2142 - val_acc: 0.9610 - val_truncated_acc: 0.9366 - val_truncated_loss: 0.3481\n",
            "-\n",
            "Input tokens:   ['кең', 'жасвар', 'қалың', 'сы', 'болған']\n",
            "Decoded tokens: ['кең', 'жасар', 'қалың', 'сы', 'болған']\n",
            "Target tokens:  ['әкең', 'жастар', 'қалың', 'осы', 'болған']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_90.h5\n",
            "Main Epoch 91/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 62s 164ms/step - loss: 0.5596 - acc: 0.8699 - truncated_acc: 0.7887 - truncated_loss: 0.9087 - val_loss: 0.2395 - val_acc: 0.9586 - val_truncated_acc: 0.9327 - val_truncated_loss: 0.3892\n",
            "-\n",
            "Input tokens:   ['отыра', 'Зөп', 'iздвенiп', 'ақ', 'қыолым']\n",
            "Decoded tokens: ['отыра', 'көп', 'iзденiп', 'ақ', 'қолым']\n",
            "Target tokens:  ['отыра', 'Көп', 'iзденiп', 'қақ', 'қолым']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_91.h5\n",
            "Main Epoch 92/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 61s 163ms/step - loss: 0.5696 - acc: 0.8664 - truncated_acc: 0.7831 - truncated_loss: 0.9247 - val_loss: 0.2023 - val_acc: 0.9623 - val_truncated_acc: 0.9388 - val_truncated_loss: 0.3287\n",
            "-\n",
            "Input tokens:   ['йатты', 'емүес', 'брiр', '–', 'отДырып']\n",
            "Decoded tokens: ['айтты', 'емес', 'берi', '–', 'отырып']\n",
            "Target tokens:  ['айтты', 'емес', 'бiр', '–', 'отырып']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_92.h5\n",
            "Main Epoch 93/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 61s 162ms/step - loss: 0.5687 - acc: 0.8648 - truncated_acc: 0.7803 - truncated_loss: 0.9235 - val_loss: 0.2325 - val_acc: 0.9584 - val_truncated_acc: 0.9324 - val_truncated_loss: 0.3777\n",
            "-\n",
            "Input tokens:   ['та', 'Буып', 'қрақ', 'әннiёң', 'iркiлмӘей']\n",
            "Decoded tokens: ['тар', 'Буып', 'қарақ', 'әнiнiң', 'iркiлмей']\n",
            "Target tokens:  ['та', 'туып', 'қарқ', 'әннiң', 'iркiлмей']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_93.h5\n",
            "Main Epoch 94/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 63s 167ms/step - loss: 0.5788 - acc: 0.8637 - truncated_acc: 0.7787 - truncated_loss: 0.9394 - val_loss: 0.2242 - val_acc: 0.9520 - val_truncated_acc: 0.9221 - val_truncated_loss: 0.3643\n",
            "-\n",
            "Input tokens:   ['тдүлкiден', 'ғаҮзиз', 'жиыны', 'Базаралыны', 'океп']\n",
            "Decoded tokens: ['түлдiкден', 'ғазиз', 'жиыны', 'Базаралыны', 'океп']\n",
            "Target tokens:  ['түлкiден', 'ғазиз', 'жиыны', 'Базаралыны', 'кеп']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_94.h5\n",
            "Main Epoch 95/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 62s 166ms/step - loss: 0.5817 - acc: 0.8659 - truncated_acc: 0.7822 - truncated_loss: 0.9445 - val_loss: 0.2211 - val_acc: 0.9594 - val_truncated_acc: 0.9340 - val_truncated_loss: 0.3592\n",
            "-\n",
            "Input tokens:   ['бОл', 'Содан', 'Абайлраға', 'қаыл', 'Ашал']\n",
            "Decoded tokens: ['бол', 'Содан', 'Абайларға', 'қалы', 'Ашал']\n",
            "Target tokens:  ['бұл', 'Содан', 'Абайларға', 'ақыл', 'шал']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_95.h5\n",
            "Main Epoch 96/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 61s 163ms/step - loss: 0.5938 - acc: 0.8588 - truncated_acc: 0.7708 - truncated_loss: 0.9641 - val_loss: 0.2131 - val_acc: 0.9602 - val_truncated_acc: 0.9354 - val_truncated_loss: 0.3463\n",
            "-\n",
            "Input tokens:   ['қаырмасына', 'сезгн', 'әсiрейе', 'таығ', 'орпысша']\n",
            "Decoded tokens: ['қарымасына', 'сезген', 'әсiрейе', 'тағы', 'орпысша']\n",
            "Target tokens:  ['қайырмасына', 'сезген', 'әсiресе', 'тағы', 'орысша']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_96.h5\n",
            "Main Epoch 97/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 62s 166ms/step - loss: 0.5547 - acc: 0.8712 - truncated_acc: 0.7909 - truncated_loss: 0.9006 - val_loss: 0.2282 - val_acc: 0.9595 - val_truncated_acc: 0.9341 - val_truncated_loss: 0.3707\n",
            "-\n",
            "Input tokens:   ['екi', 'ғажайып', 'iшю', 'бНндайда', 'үГйлер']\n",
            "Decoded tokens: ['екi', 'ғажайып', 'iшкi', 'бандайда', 'үйлер']\n",
            "Target tokens:  ['екi', 'ғажайып', 'iшi', 'бұндайда', 'үйлер']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_97.h5\n",
            "Main Epoch 98/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 62s 165ms/step - loss: 0.6044 - acc: 0.8583 - truncated_acc: 0.7699 - truncated_loss: 0.9816 - val_loss: 0.2428 - val_acc: 0.9612 - val_truncated_acc: 0.9369 - val_truncated_loss: 0.3945\n",
            "-\n",
            "Input tokens:   ['ағлысынан', 'Жиренше', 'Абжай', 'Орлабайды', 'да']\n",
            "Decoded tokens: ['алғысынан', 'Жиренше', 'Абжай', 'Орлабайды', 'да']\n",
            "Target tokens:  ['алғысынан', 'Жиренше', 'Абай', 'Оралбайды', 'да']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_98.h5\n",
            "Main Epoch 99/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 62s 166ms/step - loss: 0.6210 - acc: 0.8543 - truncated_acc: 0.7635 - truncated_loss: 1.0082 - val_loss: 0.2482 - val_acc: 0.9586 - val_truncated_acc: 0.9328 - val_truncated_loss: 0.4033\n",
            "-\n",
            "Input tokens:   ['ән', 'иненi', 'тратып', 'Ұтек', 'жылатңа']\n",
            "Decoded tokens: ['ән', 'иненi', 'таратып', 'Ұштық', 'жылата']\n",
            "Target tokens:  ['ән', 'иненi', 'тартып', 'тек', 'жылатпа']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_99.h5\n",
            "Main Epoch 100/100\n",
            "Shuffling data.\n",
            "Epoch 1/1\n",
            "375/375 [==============================] - 62s 167ms/step - loss: 0.5590 - acc: 0.8703 - truncated_acc: 0.7893 - truncated_loss: 0.9076 - val_loss: 0.2138 - val_acc: 0.9599 - val_truncated_acc: 0.9348 - val_truncated_loss: 0.3473\n",
            "-\n",
            "Input tokens:   ['ұтына', 'Аай', 'жнәе', 'еткендеһ', 'де']\n",
            "Decoded tokens: ['ұтына', 'Аай', 'және', 'еткендей', 'де']\n",
            "Target tokens:  ['ұртына', 'Абай', 'және', 'еткендей', 'де']\n",
            "-\n",
            "Saving full model to checkpoints/seq2seq_epoch_100.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hHgXjIlRRyis",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "6f9e18b8-7712-4fb1-81a0-0878d398bbcc"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "# from utils import CharacterTable, transform\n",
        "# from utils import restore_model, decode_sequences\n",
        "# from utils import read_text, tokenize\n",
        "\n",
        "error_rate = 0.6\n",
        "reverse = True\n",
        "model_path = 'checkpoints/seq2seq_epoch_100.h5'\n",
        "hidden_size = 512\n",
        "sample_mode = 'argmax'\n",
        "data_path = '/content/drive/My Drive/Colab Notebooks/kazakhBooks' \n",
        "books = ['txt1.txt','txt2.txt','txt3.txt','txt4.txt','txt5.txt',\n",
        "               'txt6.txt','txt7.txt','txt8.txt','txt9.txt']\n",
        "\n",
        "test_sentence = input(\"Enter any text: \")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    text  = read_text(data_path, books)\n",
        "    vocab = tokenize(text)\n",
        "    vocab = list(filter(None, set(vocab)))\n",
        "    # `maxlen` is the length of the longest word in the vocabulary\n",
        "    # plus two SOS and EOS characters.\n",
        "    maxlen = max([len(token) for token in vocab]) + 2\n",
        "    train_encoder, train_decoder, train_target = transform(vocab, maxlen, error_rate=error_rate, shuffle=False)\n",
        "\n",
        "    tokens = tokenize(test_sentence)\n",
        "    tokens = list(filter(None, tokens))\n",
        "    nb_tokens = len(tokens)\n",
        "    _, _, target_tokens = transform(tokens, maxlen, error_rate=error_rate, shuffle=False)\n",
        "\n",
        "    \n",
        "    input_chars = set(' '.join(train_encoder))\n",
        "    target_chars = set(' '.join(train_decoder))\n",
        "    input_ctable = CharacterTable(input_chars)\n",
        "\n",
        "    target_ctable = CharacterTable(target_chars) \n",
        "    \n",
        "    encoder_model, decoder_model = restore_model(model_path, hidden_size)\n",
        "    start_time = time.clock()\n",
        "    input_tokens, _, decoded_tokens = decode_sequences(target_tokens, target_tokens, input_ctable, target_ctable,\n",
        "        maxlen, reverse, encoder_model, decoder_model, nb_tokens,\n",
        "        sample_mode=sample_mode, random=False)\n",
        "    print(time.clock() - start_time, \"seconds\")\n",
        "    \n",
        "    print('Maybe you meant:', ' '.join([token for token in decoded_tokens]))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter any text: Ертең қанша алма жейсин\n",
            "6.418808999998873 seconds\n",
            "Maybe you meant: Ертең қанша алма жейсiн\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}