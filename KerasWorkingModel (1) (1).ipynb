{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KerasWorkingModel.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "6DWDmXOT_PSG",
        "colab_type": "code",
        "outputId": "80015e3d-67c0-49da-b4f5-b375bfcb6113",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "h2019w8o-zjH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense, Dropout\n",
        "from keras import optimizers, metrics, backend as K\n",
        "\n",
        "# For use with truncated metrics,\n",
        "# take maxlen from the validation set.\n",
        "# Hacky and hard-coded for now.\n",
        "VAL_MAXLEN = 16\n",
        "\n",
        "\n",
        "def truncated_acc(y_true, y_pred):\n",
        "    y_true = y_true[:, :VAL_MAXLEN, :]\n",
        "    y_pred = y_pred[:, :VAL_MAXLEN, :]\n",
        "    \n",
        "    acc = metrics.categorical_accuracy(y_true, y_pred)\n",
        "    return K.mean(acc, axis=-1)\n",
        "\n",
        "\n",
        "def truncated_loss(y_true, y_pred):\n",
        "    y_true = y_true[:, :VAL_MAXLEN, :]\n",
        "    y_pred = y_pred[:, :VAL_MAXLEN, :]\n",
        "    \n",
        "    loss = K.categorical_crossentropy(\n",
        "        target=y_true, output=y_pred, from_logits=False)\n",
        "    return K.mean(loss, axis=-1)\n",
        "\n",
        "\n",
        "def seq2seq(hidden_size, nb_input_chars, nb_target_chars):\n",
        "    \"\"\"Adapted from:\n",
        "    https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py\n",
        "    \"\"\"\n",
        "    \n",
        "    # Define the main model consisting of encoder and decoder.\n",
        "    encoder_inputs = Input(shape=(None, nb_input_chars),\n",
        "                           name='encoder_data')\n",
        "    encoder_lstm = LSTM(hidden_size, recurrent_dropout=0.2,\n",
        "                        return_sequences=True, return_state=False,\n",
        "                        name='encoder_lstm_1')\n",
        "    encoder_outputs = encoder_lstm(encoder_inputs)\n",
        "    \n",
        "    encoder_lstm = LSTM(hidden_size, recurrent_dropout=0.2,\n",
        "                        return_sequences=False, return_state=True,\n",
        "                        name='encoder_lstm_2')\n",
        "    encoder_outputs, state_h, state_c = encoder_lstm(encoder_outputs)\n",
        "    # We discard `encoder_outputs` and only keep the states.\n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "    # Set up the decoder, using `encoder_states` as initial state.\n",
        "    decoder_inputs = Input(shape=(None, nb_target_chars),\n",
        "                           name='decoder_data')\n",
        "    # We set up our decoder to return full output sequences,\n",
        "    # and to return internal states as well. We don't use the return\n",
        "    # states in the training model, but we will use them in inference.\n",
        "    decoder_lstm = LSTM(hidden_size, dropout=0.2, return_sequences=True,\n",
        "                        return_state=True, name='decoder_lstm')\n",
        "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
        "                                         initial_state=encoder_states)\n",
        "    decoder_softmax = Dense(nb_target_chars, activation='softmax',\n",
        "                            name='decoder_softmax')\n",
        "    decoder_outputs = decoder_softmax(decoder_outputs)\n",
        "\n",
        "    # The main model will turn `encoder_input_data` & `decoder_input_data`\n",
        "    # into `decoder_target_data`\n",
        "    model = Model(inputs=[encoder_inputs, decoder_inputs],\n",
        "                  outputs=decoder_outputs)\n",
        "    \n",
        "    adam = optimizers.Adam(lr=0.001, decay=0.0)\n",
        "    model.compile(optimizer=adam, loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy', truncated_acc, truncated_loss])\n",
        "    \n",
        "    # Define the encoder model separately.\n",
        "    encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)\n",
        "\n",
        "    # Define the decoder model separately.\n",
        "    decoder_state_input_h = Input(shape=(hidden_size,))\n",
        "    decoder_state_input_c = Input(shape=(hidden_size,))\n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "        decoder_inputs, initial_state=decoder_states_inputs)\n",
        "    decoder_states = [state_h, state_c]\n",
        "    decoder_outputs = decoder_softmax(decoder_outputs)\n",
        "    decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs,\n",
        "                          outputs=[decoder_outputs] + decoder_states)\n",
        "\n",
        "    return model, encoder_model, decoder_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ipbua1eCD3Wp",
        "colab_type": "code",
        "outputId": "5b845c5d-5636-4d15-97cc-5d1224f4b9d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install unidecode"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.6/dist-packages (1.0.23)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "neVggwvc_GQE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "import unidecode\n",
        "import numpy as np\n",
        "\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input\n",
        "\n",
        "# from model import truncated_acc, truncated_loss\n",
        "\n",
        "np.random.seed(1234)\n",
        "\n",
        "SOS = '\\t' # start of sequence.\n",
        "EOS = '*' # end of sequence.\n",
        "CHARS = list('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ ')\n",
        "REMOVE_CHARS = '[#$%\"\\+@<=>!&,-.?:;()*\\[\\]^_`{|}~/\\d\\t\\n\\r\\x0b\\x0c]'\n",
        "\n",
        "\n",
        "class CharacterTable(object):\n",
        "    \"\"\"Given a set of characters:\n",
        "    + Encode them to a one-hot integer representation\n",
        "    + Decode the one-hot integer representation to their character output\n",
        "    + Decode a vector of probabilities to their character output\n",
        "    \"\"\"\n",
        "    def __init__(self, chars):\n",
        "        \"\"\"Initialize character table.\n",
        "        # Arguments\n",
        "          chars: Characters that can appear in the input.\n",
        "        \"\"\"\n",
        "        self.chars = sorted(set(chars))\n",
        "        self.char2index = dict((c, i) for i, c in enumerate(self.chars))\n",
        "        self.index2char = dict((i, c) for i, c in enumerate(self.chars))\n",
        "        self.size = len(self.chars)\n",
        "    \n",
        "    def encode(self, C, nb_rows):\n",
        "        \"\"\"One-hot encode given string C.\n",
        "        # Arguments\n",
        "          C: string, to be encoded.\n",
        "          nb_rows: Number of rows in the returned one-hot encoding. This is\n",
        "          used to keep the # of rows for each data the same via padding.\n",
        "        \"\"\"\n",
        "        x = np.zeros((nb_rows, len(self.chars)), dtype=np.float32)\n",
        "        for i, c in enumerate(C):\n",
        "            x[i, self.char2index[c]] = 1.0\n",
        "        return x\n",
        "\n",
        "    def decode(self, x, calc_argmax=True):\n",
        "        \"\"\"Decode the given vector or 2D array to their character output.\n",
        "        # Arguments\n",
        "          x: A vector or 2D array of probabilities or one-hot encodings,\n",
        "          or a vector of character indices (used with `calc_argmax=False`).\n",
        "          calc_argmax: Whether to find the character index with maximum\n",
        "          probability, defaults to `True`.\n",
        "        \"\"\"\n",
        "        if calc_argmax:\n",
        "            indices = x.argmax(axis=-1)\n",
        "        else:\n",
        "            indices = x\n",
        "        chars = ''.join(self.index2char[ind] for ind in indices)\n",
        "        return indices, chars\n",
        "\n",
        "    def sample_multinomial(self, preds, temperature=1.0):\n",
        "        \"\"\"Sample index and character output from `preds`,\n",
        "        an array of softmax probabilities with shape (1, 1, nb_chars).\n",
        "        \"\"\"\n",
        "        # Reshaped to 1D array of shape (nb_chars,).\n",
        "        preds = np.reshape(preds, len(self.chars)).astype(np.float64)\n",
        "        preds = np.log(preds) / temperature\n",
        "        exp_preds = np.exp(preds)\n",
        "        preds = exp_preds / np.sum(exp_preds)\n",
        "        probs = np.random.multinomial(1, preds, 1)\n",
        "        index = np.argmax(probs)\n",
        "        char  = self.index2char[index]\n",
        "        return index, char\n",
        "\n",
        "\n",
        "def read_text(data_path, list_of_books):\n",
        "    text = ''\n",
        "    for book in list_of_books:\n",
        "        file_path = os.path.join(data_path, book)\n",
        "        strings = unidecode.unidecode(open(file_path).read())\n",
        "        text += strings + ' '\n",
        "    return text\n",
        "\n",
        "\n",
        "def tokenize(text):\n",
        "    tokens = [re.sub(REMOVE_CHARS, '', token)\n",
        "              for token in re.split(\"[-\\n ]\", text)]\n",
        "    return tokens\n",
        "\n",
        "    \n",
        "def add_speling_erors(token, error_rate):\n",
        "    \"\"\"Simulate some artificial spelling mistakes.\"\"\"\n",
        "    assert(0.0 <= error_rate < 1.0)\n",
        "    if len(token) < 3:\n",
        "        return token\n",
        "    rand = np.random.rand()\n",
        "    # Here are 4 different ways spelling mistakes can occur,\n",
        "    # each of which has equal chance.\n",
        "    prob = error_rate / 4.0\n",
        "    if rand < prob:\n",
        "        # Replace a character with a random character.\n",
        "        random_char_index = np.random.randint(len(token))\n",
        "        token = token[:random_char_index] + np.random.choice(CHARS) \\\n",
        "                + token[random_char_index + 1:]\n",
        "    elif prob < rand < prob * 2:\n",
        "        # Delete a character.\n",
        "        random_char_index = np.random.randint(len(token))\n",
        "        token = token[:random_char_index] + token[random_char_index + 1:]\n",
        "    elif prob * 2 < rand < prob * 3:\n",
        "        # Add a random character.\n",
        "        random_char_index = np.random.randint(len(token))\n",
        "        token = token[:random_char_index] + np.random.choice(CHARS) \\\n",
        "                + token[random_char_index:]\n",
        "    elif prob * 3 < rand < prob * 4:\n",
        "        # Transpose 2 characters.\n",
        "        random_char_index = np.random.randint(len(token) - 1)\n",
        "        token = token[:random_char_index]  + token[random_char_index + 1] \\\n",
        "                + token[random_char_index] + token[random_char_index + 2:]\n",
        "    else:\n",
        "        # No spelling errors.\n",
        "        pass\n",
        "    return token\n",
        "\n",
        "\n",
        "def transform(tokens, maxlen, error_rate=0.3, shuffle=True):\n",
        "    \"\"\"Transform tokens into model inputs and targets.\n",
        "    All inputs and targets are padded to maxlen with EOS character.\n",
        "    \"\"\"\n",
        "    if shuffle:\n",
        "        print('Shuffling data.')\n",
        "        np.random.shuffle(tokens)\n",
        "    encoder_tokens = []\n",
        "    decoder_tokens = []\n",
        "    target_tokens = []\n",
        "    for token in tokens:\n",
        "        encoder = add_speling_erors(token, error_rate=error_rate)\n",
        "        encoder += EOS * (maxlen - len(encoder)) # Padded to maxlen.\n",
        "        encoder_tokens.append(encoder)\n",
        "    \n",
        "        decoder = SOS + token\n",
        "        decoder += EOS * (maxlen - len(decoder))\n",
        "        decoder_tokens.append(decoder)\n",
        "    \n",
        "        target = decoder[1:]\n",
        "        target += EOS * (maxlen - len(target))\n",
        "        target_tokens.append(target)\n",
        "        \n",
        "        assert(len(encoder) == len(decoder) == len(target))\n",
        "    return encoder_tokens, decoder_tokens, target_tokens\n",
        "\n",
        "\n",
        "def batch(tokens, maxlen, ctable, batch_size=128, reverse=False):\n",
        "    \"\"\"Split data into chunks of `batch_size` examples.\"\"\"\n",
        "    def generate(tokens, reverse):\n",
        "        while(True): # This flag yields an infinite generator.\n",
        "            for token in tokens:\n",
        "                if reverse:\n",
        "                    token = token[::-1]\n",
        "                yield token\n",
        "    \n",
        "    token_iterator = generate(tokens, reverse)\n",
        "    data_batch = np.zeros((batch_size, maxlen, ctable.size),\n",
        "                          dtype=np.float32)\n",
        "    while(True):\n",
        "        for i in range(batch_size):\n",
        "            token = next(token_iterator)\n",
        "            data_batch[i] = ctable.encode(token, maxlen)\n",
        "        yield data_batch\n",
        "\n",
        "\n",
        "def datagen(encoder_iter, decoder_iter, target_iter):\n",
        "    \"\"\"Utility function to load data into required model format.\"\"\"\n",
        "    inputs = zip(encoder_iter, decoder_iter)\n",
        "    while(True):\n",
        "        encoder_input, decoder_input = next(inputs)\n",
        "        target = next(target_iter)\n",
        "        yield ([encoder_input, decoder_input], target)\n",
        "\n",
        "\n",
        "def decode_sequences(inputs, targets, input_ctable, target_ctable,\n",
        "                     maxlen, reverse, encoder_model, decoder_model,\n",
        "                     nb_examples, sample_mode='argmax', random=True):\n",
        "    input_tokens = []\n",
        "    target_tokens = []\n",
        "    \n",
        "    if random:\n",
        "        indices = np.random.randint(0, len(inputs), nb_examples)\n",
        "    else:\n",
        "        indices = range(nb_examples)\n",
        "        \n",
        "    for index in indices:\n",
        "        input_tokens.append(inputs[index])\n",
        "        target_tokens.append(targets[index])\n",
        "    input_sequences = batch(input_tokens, maxlen, input_ctable,\n",
        "                            nb_examples, reverse)\n",
        "    input_sequences = next(input_sequences)\n",
        "    \n",
        "    # Procedure for inference mode (sampling):\n",
        "    # 1) Encode input and retrieve initial decoder state.\n",
        "    # 2) Run one step of decoder with this initial state\n",
        "    #    and a start-of-sequence character as target.\n",
        "    #    Output will be the next target character.\n",
        "    # 3) Repeat with the current target character and current states.\n",
        "\n",
        "    # Encode the input as state vectors.    \n",
        "    states_value = encoder_model.predict(input_sequences)\n",
        "    \n",
        "    # Create batch of empty target sequences of length 1 character.\n",
        "    target_sequences = np.zeros((nb_examples, 1, target_ctable.size))\n",
        "    # Populate the first element of target sequence\n",
        "    # with the start-of-sequence character.\n",
        "    target_sequences[:, 0, target_ctable.char2index[SOS]] = 1.0\n",
        "\n",
        "    # Sampling loop for a batch of sequences.\n",
        "    # Exit condition: either hit max character limit\n",
        "    # or encounter end-of-sequence character.\n",
        "    decoded_tokens = [''] * nb_examples\n",
        "    for _ in range(maxlen):\n",
        "        # `char_probs` has shape\n",
        "        # (nb_examples, 1, nb_target_chars)\n",
        "        char_probs, h, c = decoder_model.predict(\n",
        "            [target_sequences] + states_value)\n",
        "\n",
        "        # Reset the target sequences.\n",
        "        target_sequences = np.zeros((nb_examples, 1, target_ctable.size))\n",
        "\n",
        "        # Sample next character using argmax or multinomial mode.\n",
        "        sampled_chars = []\n",
        "        for i in range(nb_examples):\n",
        "            if sample_mode == 'argmax':\n",
        "                next_index, next_char = target_ctable.decode(\n",
        "                    char_probs[i], calc_argmax=True)\n",
        "            elif sample_mode == 'multinomial':\n",
        "                next_index, next_char = target_ctable.sample_multinomial(\n",
        "                    char_probs[i], temperature=0.5)\n",
        "            else:\n",
        "                raise Exception(\n",
        "                    \"`sample_mode` accepts `argmax` or `multinomial`.\")\n",
        "            decoded_tokens[i] += next_char\n",
        "            sampled_chars.append(next_char) \n",
        "            # Update target sequence with index of next character.\n",
        "            target_sequences[i, 0, next_index] = 1.0\n",
        "\n",
        "        stop_char = set(sampled_chars)\n",
        "        if len(stop_char) == 1 and stop_char.pop() == EOS:\n",
        "            break\n",
        "            \n",
        "        # Update states.\n",
        "        states_value = [h, c]\n",
        "    \n",
        "    # Sampling finished.\n",
        "    input_tokens   = [re.sub('[%s]' % EOS, '', token)\n",
        "                      for token in input_tokens]\n",
        "    target_tokens  = [re.sub('[%s]' % EOS, '', token)\n",
        "                      for token in target_tokens]\n",
        "    decoded_tokens = [re.sub('[%s]' % EOS, '', token)\n",
        "                      for token in decoded_tokens]\n",
        "    return input_tokens, target_tokens, decoded_tokens\n",
        "\n",
        "\n",
        "def restore_model(path_to_full_model, hidden_size):\n",
        "    \"\"\"Restore model to construct the encoder and decoder.\"\"\"\n",
        "    model = load_model(path_to_full_model, custom_objects={\n",
        "        'truncated_acc': truncated_acc, 'truncated_loss': truncated_loss})\n",
        "    \n",
        "    encoder_inputs = model.input[0] # encoder_data\n",
        "    encoder_lstm1 = model.get_layer('encoder_lstm_1')\n",
        "    encoder_lstm2 = model.get_layer('encoder_lstm_2')\n",
        "    \n",
        "    encoder_outputs = encoder_lstm1(encoder_inputs)\n",
        "    _, state_h, state_c = encoder_lstm2(encoder_outputs)\n",
        "    encoder_states = [state_h, state_c]\n",
        "    encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)\n",
        "\n",
        "    decoder_inputs = model.input[1] # decoder_data\n",
        "    decoder_state_input_h = Input(shape=(hidden_size,))\n",
        "    decoder_state_input_c = Input(shape=(hidden_size,))\n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "    decoder_lstm = model.get_layer('decoder_lstm')\n",
        "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "        decoder_inputs, initial_state=decoder_states_inputs)\n",
        "    decoder_states = [state_h, state_c]\n",
        "    decoder_softmax = model.get_layer('decoder_softmax')\n",
        "    decoder_outputs = decoder_softmax(decoder_outputs)\n",
        "    decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs,\n",
        "                          outputs=[decoder_outputs] + decoder_states)\n",
        "    return encoder_model, decoder_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "73DonD0I--La",
        "colab_type": "code",
        "outputId": "384c67ea-e9b8-4e17-de7a-d2d31c6f2892",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1101
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(1234)\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "# from utils import CharacterTable, transform\n",
        "# from utils import batch, datagen, decode_sequences\n",
        "# from utils import read_text, tokenize\n",
        "# from model import seq2seq\n",
        "\n",
        "error_rate = 0.8\n",
        "hidden_size = 512\n",
        "nb_epochs = 100\n",
        "train_batch_size = 128\n",
        "val_batch_size = 256\n",
        "sample_mode = 'argmax'\n",
        "# Input sequences may optionally be reversed,\n",
        "# shown to increase performance by introducing\n",
        "# shorter term dependencies between source and target:\n",
        "# \"Learning to Execute\"\n",
        "# http://arxiv.org/abs/1410.4615\n",
        "# \"Sequence to Sequence Learning with Neural Networks\"\n",
        "# https://arxiv.org/abs/1409.3215\n",
        "reverse = True\n",
        "\n",
        "data_path = '/content/drive/My Drive/Colab Notebooks/data'\n",
        "train_books = ['nietzsche.txt', 'pride_and_prejudice.txt',\n",
        "               'shakespeare.txt', 'war_and_peace.txt']\n",
        "val_books = ['wonderland.txt']\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Prepare training data.\n",
        "    text  = read_text(data_path, train_books)\n",
        "    vocab = tokenize(text)\n",
        "    vocab = list(filter(None, set(vocab)))\n",
        "    \n",
        "    # `maxlen` is the length of the longest word in the vocabulary\n",
        "    # plus two SOS and EOS characters.\n",
        "    maxlen = max([len(token) for token in vocab]) + 2\n",
        "    train_encoder, train_decoder, train_target = transform(\n",
        "        vocab, maxlen, error_rate=error_rate, shuffle=False)\n",
        "    print(train_encoder[:10])\n",
        "    print(train_decoder[:10])\n",
        "    print(train_target[:10])\n",
        "\n",
        "    input_chars = set(' '.join(train_encoder))\n",
        "    target_chars = set(' '.join(train_decoder))\n",
        "    nb_input_chars = len(input_chars)\n",
        "    nb_target_chars = len(target_chars)\n",
        "\n",
        "    print('Size of training vocabulary =', len(vocab))\n",
        "    print('Number of unique input characters:', nb_input_chars)\n",
        "    print('Number of unique target characters:', nb_target_chars)\n",
        "    print('Max sequence length in the training set:', maxlen)\n",
        "\n",
        "    # Prepare validation data.\n",
        "    text = read_text(data_path, val_books)\n",
        "    val_tokens = tokenize(text)\n",
        "    val_tokens = list(filter(None, val_tokens))\n",
        "\n",
        "    val_maxlen = max([len(token) for token in val_tokens]) + 2\n",
        "    val_encoder, val_decoder, val_target = transform(\n",
        "        val_tokens, maxlen, error_rate=error_rate, shuffle=False)\n",
        "    print(val_encoder[:10])\n",
        "    print(val_decoder[:10])\n",
        "    print(val_target[:10])\n",
        "    print('Number of non-unique validation tokens =', len(val_tokens))\n",
        "    print('Max sequence length in the validation set:', val_maxlen)\n",
        "\n",
        "    # Define training and evaluation configuration.\n",
        "    input_ctable  = CharacterTable(input_chars)\n",
        "    target_ctable = CharacterTable(target_chars)\n",
        "\n",
        "    train_steps = len(vocab) // train_batch_size\n",
        "    val_steps = len(val_tokens) // val_batch_size\n",
        "\n",
        "    # Compile the model.\n",
        "    model, encoder_model, decoder_model = seq2seq(\n",
        "        hidden_size, nb_input_chars, nb_target_chars)\n",
        "    print(model.summary())\n",
        "\n",
        "    # Train and evaluate.\n",
        "    for epoch in range(nb_epochs):\n",
        "        print('Main Epoch {:d}/{:d}'.format(epoch + 1, nb_epochs))\n",
        "    \n",
        "        train_encoder, train_decoder, train_target = transform(\n",
        "            vocab, maxlen, error_rate=error_rate, shuffle=True)\n",
        "        \n",
        "        train_encoder_batch = batch(train_encoder, maxlen, input_ctable,\n",
        "                                    train_batch_size, reverse)\n",
        "        train_decoder_batch = batch(train_decoder, maxlen, target_ctable,\n",
        "                                    train_batch_size)\n",
        "        train_target_batch  = batch(train_target, maxlen, target_ctable,\n",
        "                                    train_batch_size)    \n",
        "\n",
        "        val_encoder_batch = batch(val_encoder, maxlen, input_ctable,\n",
        "                                  val_batch_size, reverse)\n",
        "        val_decoder_batch = batch(val_decoder, maxlen, target_ctable,\n",
        "                                  val_batch_size)\n",
        "        val_target_batch  = batch(val_target, maxlen, target_ctable,\n",
        "                                  val_batch_size)\n",
        "    \n",
        "        train_loader = datagen(train_encoder_batch,\n",
        "                               train_decoder_batch, train_target_batch)\n",
        "        val_loader = datagen(val_encoder_batch,\n",
        "                             val_decoder_batch, val_target_batch)\n",
        "    \n",
        "        model.fit_generator(train_loader,\n",
        "                            steps_per_epoch=train_steps,\n",
        "                            epochs=1, verbose=1,\n",
        "                            validation_data=val_loader,\n",
        "                            validation_steps=val_steps)\n",
        "\n",
        "        # On epoch end - decode a batch of misspelled tokens from the\n",
        "        # validation set to visualize speller performance.\n",
        "        nb_tokens = 5\n",
        "        input_tokens, target_tokens, decoded_tokens = decode_sequences(\n",
        "            val_encoder, val_target, input_ctable, target_ctable,\n",
        "            maxlen, reverse, encoder_model, decoder_model, nb_tokens,\n",
        "            sample_mode=sample_mode, random=True)\n",
        "        \n",
        "        print('-')\n",
        "        print('Input tokens:  ', input_tokens)\n",
        "        print('Decoded tokens:', decoded_tokens)\n",
        "        print('Target tokens: ', target_tokens)\n",
        "        print('-')\n",
        "        \n",
        "        # Save the model at end of each epoch.\n",
        "        model_file = '_'.join(['seq2seq', 'epoch', str(epoch + 1)]) + '.h5'\n",
        "        save_dir = 'checkpoints'\n",
        "        if not os.path.exists(save_dir):\n",
        "            os.makedirs(save_dir)\n",
        "        save_path = os.path.join(save_dir, model_file)\n",
        "        print('Saving full model to {:s}'.format(save_path))\n",
        "        model.save(save_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['demm******************', \"cmoes't***************\", 'unexposde*************', 'angEed****************', 'Sighs*****************', 'commonplaceness*******', 'eadiness**************', 'jbluish***************', 'thee******************', 'faWsified*************']\n",
            "['\\tdeem*****************', \"\\tcomes't**************\", '\\tunexposed************', '\\tangled***************', '\\tSighs****************', '\\tcommonplaceness******', '\\treadiness************', '\\tbluish***************', '\\tthee*****************', '\\tfalsified************']\n",
            "['deem******************', \"comes't***************\", 'unexposed*************', 'angled****************', 'Sighs*****************', 'commonplaceness*******', 'readiness*************', 'bluish****************', 'thee******************', 'falsified*************']\n",
            "Size of training vocabulary = 33043\n",
            "Number of unique input characters: 55\n",
            "Number of unique target characters: 56\n",
            "Max sequence length in the training set: 22\n",
            "[\"ALICE'tS**************\", 'ADVENoTURES***********', 'IN********************', 'WONDERLAND************', 'Lweis*****************', 'Crroll****************', 'TeE*******************', 'MILLTENNIUM***********', 'FULCRUM***************', 'EIDTION***************']\n",
            "[\"\\tALICE'S**************\", '\\tADVENTURES***********', '\\tIN*******************', '\\tWONDERLAND***********', '\\tLewis****************', '\\tCarroll**************', '\\tTHE******************', '\\tMILLENNIUM***********', '\\tFULCRUM**************', '\\tEDITION**************']\n",
            "[\"ALICE'S***************\", 'ADVENTURES************', 'IN********************', 'WONDERLAND************', 'Lewis*****************', 'Carroll***************', 'THE*******************', 'MILLENNIUM************', 'FULCRUM***************', 'EDITION***************']\n",
            "Number of non-unique validation tokens = 26763\n",
            "Max sequence length in the validation set: 16\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "encoder_data (InputLayer)       (None, None, 55)     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "encoder_lstm_1 (LSTM)           (None, None, 512)    1163264     encoder_data[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "decoder_data (InputLayer)       (None, None, 56)     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "encoder_lstm_2 (LSTM)           [(None, 512), (None, 2099200     encoder_lstm_1[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "decoder_lstm (LSTM)             [(None, None, 512),  1165312     decoder_data[0][0]               \n",
            "                                                                 encoder_lstm_2[0][1]             \n",
            "                                                                 encoder_lstm_2[0][2]             \n",
            "__________________________________________________________________________________________________\n",
            "decoder_softmax (Dense)         (None, None, 56)     28728       decoder_lstm[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 4,456,504\n",
            "Trainable params: 4,456,504\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Main Epoch 1/100\n",
            "Shuffling data.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-daeb7e51c2c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         train_encoder, train_decoder, train_target = transform(\n\u001b[0;32m---> 89\u001b[0;31m             vocab, maxlen, error_rate=error_rate, shuffle=True)\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         train_encoder_batch = batch(train_encoder, maxlen, input_ctable,\n",
            "\u001b[0;32m<ipython-input-65-4a8c0c6176d9>\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(tokens, maxlen, error_rate, shuffle)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0mtarget_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_speling_erors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0mencoder\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mEOS\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmaxlen\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Padded to maxlen.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mencoder_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-65-4a8c0c6176d9>\u001b[0m in \u001b[0;36madd_speling_erors\u001b[0;34m(token, error_rate)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mrand\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;31m# Add a random character.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mrandom_char_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mrandom_char_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHARS\u001b[0m\u001b[0;34m)\u001b[0m                 \u001b[0;34m+\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandom_char_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mrand\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "YclSrUgi9DsZ",
        "colab_type": "code",
        "outputId": "d3c0c38b-f5ba-450a-ac15-d124563c360d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "# from utils import CharacterTable, transform\n",
        "# from utils import restore_model, decode_sequences\n",
        "# from utils import read_text, tokenize\n",
        "\n",
        "error_rate = 0.6\n",
        "reverse = True\n",
        "model_path = 'checkpoints/seq2seq_epoch_100.h5'\n",
        "hidden_size = 512\n",
        "sample_mode = 'argmax'\n",
        "data_path = '/content/drive/My Drive/Colab Notebooks/data'\n",
        "books = ['nietzsche.txt', 'pride_and_prejudice.txt', 'shakespeare.txt', 'war_and_peace.txt']\n",
        "\n",
        "test_sentence = input(\"Enter any text: \")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    text  = read_text(data_path, books)\n",
        "    vocab = tokenize(text)\n",
        "    vocab = list(filter(None, set(vocab)))\n",
        "    # `maxlen` is the length of the longest word in the vocabulary\n",
        "    # plus two SOS and EOS characters.\n",
        "    maxlen = max([len(token) for token in vocab]) + 2\n",
        "    train_encoder, train_decoder, train_target = transform(vocab, maxlen, error_rate=error_rate, shuffle=False)\n",
        "\n",
        "    tokens = tokenize(test_sentence)\n",
        "    tokens = list(filter(None, tokens))\n",
        "    nb_tokens = len(tokens)\n",
        "    _, _, target_tokens = transform(tokens, maxlen, error_rate=error_rate, shuffle=False)\n",
        "\n",
        "    \n",
        "    input_chars = set(' '.join(train_encoder))\n",
        "    target_chars = set(' '.join(train_decoder))\n",
        "    input_ctable = CharacterTable(input_chars)\n",
        "\n",
        "    target_ctable = CharacterTable(target_chars) \n",
        "    \n",
        "    encoder_model, decoder_model = restore_model(model_path, hidden_size)\n",
        "    start_time = time.clock()\n",
        "    input_tokens, _, decoded_tokens = decode_sequences(target_tokens, target_tokens, input_ctable, target_ctable,\n",
        "        maxlen, reverse, encoder_model, decoder_model, nb_tokens,\n",
        "        sample_mode=sample_mode, random=False)\n",
        "    print(time.clock() - start_time, \"seconds\")\n",
        "    \n",
        "    print('Maybe you meant:', ' '.join([token for token in decoded_tokens]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter any text: This repository cnotains a Keras mplementation of an encoder-decoder LSTM architecture for sequence-to-sequence speling correction\n",
            "26.847794999999678 seconds\n",
            "Maybe you meant: This repository contains a Keras implementation of an encoeder decoder LIST architecture for sequence to sequence spelling correction\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Dvn50TyTd5wT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# model.save('dd.h5')\n",
        "# model.load('seq2seq.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gh2DpEaM9e89",
        "colab_type": "code",
        "outputId": "fd51bb0a-e3c1-4244-9bf5-5a4fee09c7b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "# from utils import CharacterTable, transform\n",
        "# from utils import restore_model, decode_sequences\n",
        "# from utils import read_text, tokenize\n",
        "# This repository cnotains a Keras mplementation of an encoder-decoder LSTM architecture for sequence-to-sequence speling correction\n",
        "error_rate = 0.6\n",
        "reverse = True\n",
        "model_path = 'checkpoints/seq2seq_epoch_100.h5'\n",
        "hidden_size = 512\n",
        "sample_mode = 'argmax'\n",
        "data_path = '/content/drive/My Drive/Colab Notebooks/data'\n",
        "books = ['nietzsche.txt', 'pride_and_prejudice.txt', 'shakespeare.txt', 'war_and_peace.txt']\n",
        "start_time = time.clock()\n",
        "test_sentence = input(\"Enter any text: \")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    text  = read_text(data_path, books)\n",
        "    vocab = tokenize(text)\n",
        "    vocab = list(filter(None, set(vocab)))\n",
        "    # `maxlen` is the length of the longest word in the vocabulary\n",
        "    # plus two SOS and EOS characters.\n",
        "    maxlen = max([len(token) for token in vocab]) + 2\n",
        "    train_encoder, train_decoder, train_target = transform(vocab, maxlen, error_rate=error_rate, shuffle=False)\n",
        "\n",
        "    tokens = tokenize(test_sentence)\n",
        "    tokens = list(filter(None, tokens))\n",
        "    nb_tokens = len(tokens)\n",
        "    _, _, target_tokens = transform(tokens, maxlen, error_rate=error_rate, shuffle=False)\n",
        "\n",
        "    \n",
        "    input_chars = set(' '.join(train_encoder))\n",
        "    target_chars = set(' '.join(train_decoder))\n",
        "    input_ctable = CharacterTable(input_chars)\n",
        "\n",
        "    target_ctable = CharacterTable(target_chars) \n",
        "    encoder_model, decoder_model = restore_model(model_path, hidden_size)\n",
        "    \n",
        "    input_tokens, _, decoded_tokens = decode_sequences(\n",
        "        target_tokens, target_tokens, input_ctable, target_ctable,\n",
        "        maxlen, reverse, encoder_model, decoder_model, nb_tokens,\n",
        "        sample_mode=sample_mode, random=False)\n",
        "    \n",
        "    \n",
        "    print('Maybe you meant:', ' '.join([token for token in decoded_tokens]))\n",
        "    print(time.clock() - start_time, \"seconds\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n",
            "Input sentence:   How to get to Othe paNrkI do not naw\n",
            "-\n",
            "Decoded sentence: How to get to Othe parks do not naw\n",
            "-\n",
            "Target sentence:  How to get to the parkI do not knaw\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}